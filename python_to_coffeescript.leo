<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<?xml-stylesheet ekr_test ?>
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20160220051058.1"><vh>Startup</vh>
<v t="ekr.20160220051359.1"><vh>@settings</vh>
<v t="ekr.20160220051359.2"><vh>@data history-list</vh></v>
<v t="ekr.20160220051359.4"><vh>@bool preload-find-pattern = False</vh></v>
</v>
<v t="ekr.20160220051417.1"><vh>Scripts</vh>
<v t="ekr.20160220051417.2"><vh>@button write-unit-tests</vh>
<v t="ekr.20160220051417.3"><vh>&lt;&lt; docstring &gt;&gt; (write-unit-tests)</vh></v>
<v t="ekr.20160220051417.4"><vh>class TestWriter</vh>
<v t="ekr.20160220051417.5"><vh>&lt;&lt; define file_template &gt;&gt;</vh></v>
<v t="ekr.20160220051417.6"><vh>&lt;&lt; define test_template &gt;&gt;</vh></v>
<v t="ekr.20160220051417.7"><vh> ctor</vh></v>
<v t="ekr.20160220051417.8"><vh>clean</vh></v>
<v t="ekr.20160220051417.9"><vh>get_body</vh></v>
<v t="ekr.20160220051417.10"><vh>run</vh></v>
<v t="ekr.20160220051417.11"><vh>plural</vh></v>
<v t="ekr.20160220051417.12"><vh>test</vh></v>
<v t="ekr.20160220051417.13"><vh>write_file</vh></v>
</v>
</v>
<v t="ekr.20160220051417.14"><vh>@button check-leading-lines</vh></v>
</v>
<v t="ekr.20160220051105.1"><vh>Unused</vh>
<v t="ekr.20160220081725.1"><vh>ast_utils.py</vh>
<v t="ekr.20160220081959.1"><vh>&lt;&lt; imports &gt;&gt;</vh></v>
<v t="ekr.20160220093908.1"><vh>class Bunch (Python Cookbook)</vh></v>
<v t="ekr.20160220081959.9"><vh>mark_text_ranges &amp; helpers</vh>
<v t="ekr.20160220084232.1"><vh>&lt;&lt; mark_text_ranges helpers &gt;&gt;</vh>
<v t="ekr.20160220084252.1"><vh>_extract_tokens</vh></v>
<v t="ekr.20160220084252.2"><vh>_mark_text_ranges_rec</vh></v>
<v t="ekr.20160220084252.3"><vh>_strip_trailing_junk_from_expressions</vh></v>
<v t="ekr.20160220084252.4"><vh>_strip_trailing_extra_closers</vh></v>
<v t="ekr.20160220084252.5"><vh>_strip_unclosed_brackets</vh></v>
<v t="ekr.20160220084252.6"><vh>_mark_end_and_return_child_tokens</vh></v>
</v>
<v t="ekr.20160220081959.14"><vh>_get_ordered_child_nodes</vh></v>
<v t="ekr.20160220081959.15"><vh>_tokens_text</vh></v>
<v t="ekr.20160220081959.11"><vh>fix_ast_problems</vh></v>
<v t="ekr.20160220081959.12"><vh>compare_node_positions</vh></v>
</v>
</v>
<v t="ekr.20160221033247.1"><vh>stat vars</vh></v>
<v t="ekr.20160220103223.39"><vh>cst.print_stats</vh></v>
<v t="ekr.20160220103223.21"><vh>cst.gen_arg_start &amp; gen_arg_end (not used)</vh></v>
<v t="ekr.20160220050433.188"><vh>class TestClass</vh>
<v t="ekr.20160220050433.189"><vh>parse_group (Guido)</vh></v>
<v t="ekr.20160220050433.190"><vh>return_all</vh></v>
<v t="ekr.20160220050433.191"><vh>return_array</vh></v>
<v t="ekr.20160220050433.192"><vh>return_list</vh></v>
<v t="ekr.20160220050433.193"><vh>return_two_lists (fails)</vh></v>
</v>
</v>
</v>
<v t="ekr.20160222041249.1"><vh>Docs</vh>
<v t="ekr.20160220050053.2"><vh>@clean README.md</vh>
<v t="ekr.20160221071334.1"><vh>Overview</vh></v>
<v t="ekr.20160221071354.1"><vh>Rationale</vh></v>
<v t="ekr.20160221070924.1"><vh>Quick start</vh></v>
<v t="ekr.20160221071222.1"><vh>Command-line arguments</vh></v>
<v t="ekr.20160221071408.1"><vh>Code notes</vh></v>
<v t="ekr.20160221071735.1"><vh>Summary</vh></v>
</v>
<v t="ekr.20160222040422.1"><vh>@clean design.md</vh></v>
<v t="ekr.20160222030015.1"><vh>2.1.5 Explicit line joining</vh></v>
<v t="ekr.20160222030107.1"><vh>ast differences</vh></v>
</v>
<v t="ekr.20160220050321.1"><vh>@clean python_to_coffeescript.py</vh>
<v t="ekr.20160220050745.1"><vh>  &lt;&lt; license &gt;&gt; (python_to_coffeescript.py)</vh></v>
<v t="ekr.20160220050433.2"><vh>  &lt;&lt; imports &gt;&gt; (python_to_coffeescript.py)</vh></v>
<v t="ekr.20160220050433.11"><vh>  main</vh></v>
<v t="ekr.20160220050433.7"><vh>  utility functions</vh>
<v t="ekr.20160220050433.8"><vh>dump</vh></v>
<v t="ekr.20160220050433.9"><vh>dump_dict</vh></v>
<v t="ekr.20160220050433.10"><vh>dump_list</vh></v>
<v t="ekr.20160220050433.12"><vh>pdb</vh></v>
<v t="ekr.20160220050433.13"><vh>truncate</vh></v>
</v>
<v t="ekr.20160220103223.1"><vh>class CoffeeScriptTokenizer</vh>
<v t="ekr.20160220103223.2"><vh>class OutputToken</vh></v>
<v t="ekr.20160221024549.1"><vh>class StateStack</vh>
<v t="ekr.20160220175541.1"><vh>ss.get</vh></v>
<v t="ekr.20160220151109.1"><vh>ss.has</vh></v>
<v t="ekr.20160221025511.1"><vh>ss.pop</vh></v>
<v t="ekr.20160220103223.40"><vh>ss.push</vh></v>
<v t="ekr.20160220174952.1"><vh>ss.remove</vh></v>
</v>
<v t="ekr.20160220103223.4"><vh> ct.ctor</vh></v>
<v t="ekr.20160220103223.7"><vh> ct.format</vh></v>
<v t="ekr.20160220103223.8"><vh>ct.Input token Handlers</vh>
<v t="ekr.20160220103223.9"><vh>ct.do_comment</vh></v>
<v t="ekr.20160220103223.10"><vh>ct.do_endmarker</vh></v>
<v t="ekr.20160220103223.11"><vh>ct.do_errortoken</vh></v>
<v t="ekr.20160220103223.12"><vh>ct.do_indent &amp; do_dedent</vh></v>
<v t="ekr.20160220103223.13"><vh>ct.do_name</vh></v>
<v t="ekr.20160220103223.14"><vh>ct.do_newline</vh></v>
<v t="ekr.20160220103223.15"><vh>ct.do_nl</vh></v>
<v t="ekr.20160220103223.16"><vh>ct.do_number</vh></v>
<v t="ekr.20160220103223.17"><vh>ct.do_op</vh></v>
<v t="ekr.20160220103223.18"><vh>ct.do_string</vh></v>
</v>
<v t="ekr.20160220103223.19"><vh>ct.Output token generators</vh>
<v t="ekr.20160220103223.20"><vh>ct.add_token</vh></v>
<v t="ekr.20160220103223.25"><vh>ct.clean</vh></v>
<v t="ekr.20160220103223.26"><vh>ct.clean_blank_lines</vh></v>
<v t="ekr.20160220152030.1"><vh>ct.gen_at</vh></v>
<v t="ekr.20160220103223.22"><vh>ct.gen_backslash</vh></v>
<v t="ekr.20160220103223.23"><vh>ct.gen_blank</vh></v>
<v t="ekr.20160220103223.24"><vh>ct.gen_blank_lines</vh></v>
<v t="ekr.20160220143251.1"><vh>ct.gen_class_or_def</vh></v>
<v t="ekr.20160220143853.1"><vh>ct.gen_close_paren</vh></v>
<v t="ekr.20160220143629.1"><vh>ct.gen_colon</vh></v>
<v t="ekr.20160220160809.1"><vh>ct.gen_comma</vh></v>
<v t="ekr.20160220103223.27"><vh>ct.gen_file_start &amp; gen_file_end</vh></v>
<v t="ekr.20160220144848.1"><vh>ct.gen_import</vh></v>
<v t="ekr.20160220103223.28"><vh>ct.gen_line_indent</vh></v>
<v t="ekr.20160220103223.29"><vh>ct.gen_line_start &amp; gen_line_end</vh></v>
<v t="ekr.20160220103223.30"><vh>ct.gen_lt &amp; gen_rt</vh></v>
<v t="ekr.20160220103223.31"><vh>ct.gen_op*</vh></v>
<v t="ekr.20160220143842.1"><vh>ct.gen_open_paren</vh></v>
<v t="ekr.20160220151922.1"><vh>ct.gen_period</vh></v>
<v t="ekr.20160220103223.32"><vh>ct.gen_possible_unary_op &amp; gen_unary_op</vh></v>
<v t="ekr.20160220151729.1"><vh>ct.gen_self</vh></v>
<v t="ekr.20160220103223.33"><vh>ct.gen_star_op</vh></v>
<v t="ekr.20160220103223.34"><vh>ct.gen_star_star_op</vh></v>
<v t="ekr.20160220103223.35"><vh>ct.gen_word &amp; gen_word_op</vh></v>
</v>
</v>
<v t="ekr.20160220050433.14"><vh>class CoffeeScriptTraverser</vh>
<v t="ekr.20160220050433.16"><vh> cv.format</vh></v>
<v t="ekr.20160220050433.82"><vh> cv.indent</vh></v>
<v t="ekr.20160220050433.17"><vh> cv.visit</vh></v>
<v t="ekr.20160220050433.18"><vh>cv.Contexts</vh>
<v t="ekr.20160220050433.19"><vh>cv.ClassDef</vh></v>
<v t="ekr.20160220050433.20"><vh>cv.FunctionDef</vh></v>
<v t="ekr.20160220050433.21"><vh>cv.Interactive</vh></v>
<v t="ekr.20160220050433.22"><vh>cv.Module</vh></v>
<v t="ekr.20160220050433.23"><vh>cv.Lambda</vh></v>
</v>
<v t="ekr.20160220050433.24"><vh>cv.Expressions</vh>
<v t="ekr.20160220050433.25"><vh>cv.Expr</vh></v>
<v t="ekr.20160220050433.26"><vh>cv.Expression</vh></v>
<v t="ekr.20160220050433.27"><vh>cv.GeneratorExp</vh></v>
<v t="ekr.20160220050433.28"><vh>cv.ctx nodes</vh></v>
</v>
<v t="ekr.20160220050433.29"><vh>cv.Operands</vh>
<v t="ekr.20160220050433.30"><vh>cv.arguments</vh></v>
<v t="ekr.20160220050433.31"><vh>cv.arg (Python3 only)</vh></v>
<v t="ekr.20160220050433.32"><vh>cv.Attribute</vh></v>
<v t="ekr.20160220050433.33"><vh>cv.Bytes</vh></v>
<v t="ekr.20160220050433.34"><vh>cv.Call &amp; cv.keyword</vh>
<v t="ekr.20160220050433.35"><vh>cv.keyword</vh></v>
</v>
<v t="ekr.20160220050433.36"><vh>cv.comprehension</vh></v>
<v t="ekr.20160220050433.37"><vh>cv.Dict</vh></v>
<v t="ekr.20160220050433.38"><vh>cv.Ellipsis</vh></v>
<v t="ekr.20160220050433.39"><vh>cv.ExtSlice</vh></v>
<v t="ekr.20160220050433.40"><vh>cv.Index</vh></v>
<v t="ekr.20160220050433.41"><vh>cv.List</vh></v>
<v t="ekr.20160220050433.42"><vh>cv.ListComp</vh></v>
<v t="ekr.20160220050433.43"><vh>cv.Name &amp; cv.NameConstant</vh></v>
<v t="ekr.20160220050433.44"><vh>cv.Num</vh></v>
<v t="ekr.20160220050433.45"><vh>cv.Repr</vh></v>
<v t="ekr.20160220050433.46"><vh>cv.Slice</vh></v>
<v t="ekr.20160220050433.47"><vh>cv.Str</vh></v>
<v t="ekr.20160220050433.48"><vh>cv.Subscript</vh></v>
<v t="ekr.20160220050433.49"><vh>cv.Tuple</vh></v>
</v>
<v t="ekr.20160220050433.50"><vh>cv.Operators</vh>
<v t="ekr.20160220050433.83"><vh> cv.op_name</vh></v>
<v t="ekr.20160220050433.51"><vh>cv.BinOp</vh></v>
<v t="ekr.20160220050433.52"><vh>cv.BoolOp</vh></v>
<v t="ekr.20160220050433.53"><vh>cv.Compare</vh></v>
<v t="ekr.20160220050433.55"><vh>cv.ifExp (ternary operator)</vh></v>
<v t="ekr.20160220050433.54"><vh>cv.UnaryOp</vh></v>
</v>
<v t="ekr.20160220050433.56"><vh>cv.Statements</vh>
<v t="ekr.20160220050433.57"><vh>cv.Assert</vh></v>
<v t="ekr.20160220050433.58"><vh>cv.Assign</vh></v>
<v t="ekr.20160220050433.59"><vh>cv.AugAssign</vh></v>
<v t="ekr.20160220050433.60"><vh>cv.Break</vh></v>
<v t="ekr.20160220050433.61"><vh>cv.Continue</vh></v>
<v t="ekr.20160220050433.62"><vh>cv.Delete</vh></v>
<v t="ekr.20160220050433.63"><vh>cv.ExceptHandler</vh></v>
<v t="ekr.20160220050433.64"><vh>cv.Exec</vh></v>
<v t="ekr.20160220050433.65"><vh>cv.For</vh></v>
<v t="ekr.20160220050433.66"><vh>cv.Global</vh></v>
<v t="ekr.20160220050433.67"><vh>cv.If</vh></v>
<v t="ekr.20160220050433.68"><vh>cv.Import &amp; helper</vh>
<v t="ekr.20160220050433.69"><vh>cv.get_import_names</vh></v>
</v>
<v t="ekr.20160220050433.70"><vh>cv.ImportFrom</vh></v>
<v t="ekr.20160220050433.71"><vh>cv.Pass</vh></v>
<v t="ekr.20160220050433.72"><vh>cv.Print</vh></v>
<v t="ekr.20160220050433.73"><vh>cv.Raise</vh></v>
<v t="ekr.20160220050433.74"><vh>cv.Return</vh></v>
<v t="ekr.20160220094909.1"><vh>cv.Try</vh></v>
<v t="ekr.20160220050433.75"><vh>cv.TryExcept</vh></v>
<v t="ekr.20160220050433.76"><vh>cv.TryFinally</vh></v>
<v t="ekr.20160220050433.77"><vh>cv.While</vh></v>
<v t="ekr.20160220050433.78"><vh>cv.With</vh></v>
<v t="ekr.20160220050433.79"><vh>cv.Yield</vh></v>
</v>
</v>
<v t="ekr.20160220050433.86"><vh>class LeoGlobals</vh>
<v t="ekr.20160220050433.87"><vh>class NullObject (Python Cookbook)</vh></v>
<v t="ekr.20160220110252.1"><vh>class ReadLinesClass</vh></v>
<v t="ekr.20160220050433.88"><vh>g._callerName</vh></v>
<v t="ekr.20160220050433.89"><vh>g.callers</vh></v>
<v t="ekr.20160220050433.90"><vh>g.cls</vh></v>
<v t="ekr.20160220104316.1"><vh>g.computeLeadingWhitespace</vh></v>
<v t="ekr.20160220105138.1"><vh>g.computeLeadingWhitespaceWidth</vh></v>
<v t="ekr.20160220104209.1"><vh>g.isString &amp; isUnicode</vh></v>
<v t="ekr.20160220050433.91"><vh>g.pdb</vh></v>
<v t="ekr.20160220050433.92"><vh>g.shortFileName</vh></v>
<v t="ekr.20160220050433.93"><vh>g.splitLines</vh></v>
<v t="ekr.20160220104523.1"><vh>g.toUnicode</vh></v>
<v t="ekr.20160220050433.94"><vh>g.trace</vh></v>
<v t="ekr.20160220104732.1"><vh>g.u &amp; g.ue</vh></v>
</v>
<v t="ekr.20160220050433.118"><vh>class MakeCoffeeScriptController</vh>
<v t="ekr.20160220050433.119"><vh>mcs.ctor</vh></v>
<v t="ekr.20160220050433.120"><vh>mcs.finalize</vh></v>
<v t="ekr.20160220050433.121"><vh>mcs.make_coffeescript_file</vh></v>
<v t="ekr.20160220055816.3"><vh>mcs.output_time_stamp</vh></v>
<v t="ekr.20160220050433.122"><vh>mcs.run</vh></v>
<v t="ekr.20160220050433.123"><vh>mcs.run_all_unit_tests</vh></v>
<v t="ekr.20160220050433.124"><vh>mcs.scan_command_line</vh></v>
<v t="ekr.20160220050433.125"><vh>mcs.scan_options &amp; helpers</vh>
<v t="ekr.20160220050433.127"><vh>mcs.create_parser</vh></v>
<v t="ekr.20160220050433.129"><vh>mcs.get_config_string</vh></v>
<v t="ekr.20160220050433.130"><vh>mcs.init_parser</vh></v>
<v t="ekr.20160220050433.131"><vh>mcs.is_section_name</vh></v>
</v>
</v>
<v t="ekr.20160220103223.3"><vh>class ParseState</vh></v>
<v t="ekr.20160221143000.1"><vh>class TokenSync</vh>
<v t="ekr.20160221143402.1"><vh> cv.ctor</vh></v>
<v t="ekr.20160221124544.1"><vh>cv.advance (test)</vh></v>
<v t="ekr.20160221144446.1"><vh>cv.do_backslash (maybe)</vh></v>
<v t="ekr.20160221143755.1"><vh>cv.do_line_indent (maybe)</vh></v>
<v t="ekr.20160221163955.1"><vh>cv.dump_queue &amp; enqueue</vh></v>
<v t="ekr.20160221151640.1"><vh>cv.sync_op</vh></v>
<v t="ekr.20160221145254.1"><vh>cv.sync_word</vh></v>
</v>
</v>
<v t="ekr.20160222023117.1"><vh>@clean test.py</vh></v>
<v t="ekr.20160220131008.1"><vh>** To do</vh></v>
<v t="ekr.20160221042509.1"><vh>recent</vh>
<v t="ekr.20160220103223.4"></v>
<v t="ekr.20160220103223.20"></v>
<v t="ekr.20160220103223.30"></v>
<v t="ekr.20160220143853.1"></v>
<v t="ekr.20160220143842.1"></v>
<v t="ekr.20160220050433.121"></v>
</v>
<v t="ekr.20160222023712.1"><vh>recent2</vh>
<v t="ekr.20160220103223.1"></v>
<v t="ekr.20160220050433.14"></v>
<v t="ekr.20160221143000.1"></v>
<v t="ekr.20160220050433.18"></v>
<v t="ekr.20160220050433.56"></v>
<v t="ekr.20160221124544.1"></v>
<v t="ekr.20160220050433.34"></v>
<v t="ekr.20160220050433.19"></v>
<v t="ekr.20160220050433.20"></v>
</v>
</vnodes>
<tnodes>
<t tx="ekr.20160220050053.2" markdown-import="7d7100550e756e6465726c696e655f6469637471017d7102581700000020707974686f6e2d746f2d636f666665657363726970747103550123710473732e">@language md
@wrap

This is the readme file for python_to_coffeescript.py. It explains what
the script does, why I created it, and how to use the script. A last section explains why the code is as it is and how it may evolve. Full source code for the script is in its [github repository](https://github.com/edreamleo/python-to-coffeescript). This script is offered under the terms of [Leo's MIT License](http://leoeditor.com/license.html).

@others</t>
<t tx="ekr.20160220050321.1">#!/usr/bin/env python
'''
This script makes a coffeescript file for every python source file listed
on the command line (wildcard file names are supported).

For full details, see README.md.

Released under the MIT Licence.

Written by Edward K. Ream.
'''
&lt;&lt; license &gt;&gt;
&lt;&lt; imports &gt;&gt;
use_tree = False
@others

g = LeoGlobals() # For ekr.
if __name__ == "__main__":
    main()
</t>
<t tx="ekr.20160220050433.10">
def dump_list(title, aList):
    '''Dump a list with a header.'''
    dump(title)
    for z in aList:
        print(z)
    print('')
</t>
<t tx="ekr.20160220050433.11">
def main():
    '''
    The driver for the stand-alone version of make-stub-files.
    All options come from ~/stubs/make_stub_files.cfg.
    '''
    # g.cls()
    controller = MakeCoffeeScriptController()
    controller.scan_command_line()
    controller.scan_options()
    controller.run()
    print('done')
</t>
<t tx="ekr.20160220050433.118">

class MakeCoffeeScriptController(object):
    '''The controller class for python_to_coffeescript.py.'''

    @others
</t>
<t tx="ekr.20160220050433.119">
def __init__(self):
    '''Ctor for MakeCoffeeScriptController class.'''
    self.options = {}
    # Ivars set on the command line...
    self.config_fn = None
    self.enable_unit_tests = False
    self.files = [] # May also be set in the config file.
    self.section_names = ('Global',)
    # Ivars set in the config file...
    self.output_directory = self.finalize('.')
    self.overwrite = False
    self.verbose = False # Trace config arguments.
</t>
<t tx="ekr.20160220050433.12">
def pdb(self):
    '''Invoke a debugger during unit testing.'''
    print('pdb')
    # Avoid try/except during development.
    # try:
        # import leo.core.leoGlobals as leo_g
        # leo_g.pdb()
    # except ImportError:
        # import pdb
        # pdb.set_trace()
</t>
<t tx="ekr.20160220050433.120">
def finalize(self, fn):
    '''Finalize and regularize a filename.'''
    fn = os.path.expanduser(fn)
    fn = os.path.abspath(fn)
    fn = os.path.normpath(fn)
    return fn
</t>
<t tx="ekr.20160220050433.121">
def make_coffeescript_file(self, fn):
    '''
    Make a stub file in the output directory for all source files mentioned
    in the [Source Files] section of the configuration file.
    '''
    if not fn.endswith('.py'):
        print('not a python file', fn)
        return
    if not os.path.exists(fn):
        print('not found', fn)
        return
    base_fn = os.path.basename(fn)
    out_fn = os.path.join(self.output_directory, base_fn)
    out_fn = os.path.normpath(out_fn)
    out_fn = out_fn[: -3] + '.coffee'
    dir_ = os.path.dirname(out_fn)
    if os.path.exists(out_fn) and not self.overwrite:
        print('file exists: %s' % out_fn)
    elif not dir_ or os.path.exists(dir_):
        t1 = time.clock()
        s = open(fn).read()
        readlines = g.ReadLinesClass(s).next
        tokens = list(tokenize.generate_tokens(readlines))
        if use_tree:
            node = ast.parse(s, filename=fn, mode='exec')
            s = CoffeeScriptTraverser(controller=self).format(node, tokens)
        else:
            s = CoffeeScriptTokenizer(controller=self).format(tokens)
        f = open(out_fn, 'w')
        self.output_time_stamp(f)
        f.write(s)
        f.close()
        print('wrote: %s' % out_fn)
    else:
        print('output directory not not found: %s' % dir_)
</t>
<t tx="ekr.20160220050433.122">
def run(self):
    '''
    Make stub files for all files.
    Do nothing if the output directory does not exist.
    '''
    if self.enable_unit_tests:
        self.run_all_unit_tests()
    if self.files:
        dir_ = self.output_directory
        if dir_:
            if os.path.exists(dir_):
                for fn in self.files:
                    self.make_coffeescript_file(fn)
            else:
                print('output directory not found: %s' % dir_)
        else:
            print('no output directory')
    elif not self.enable_unit_tests:
        print('no input files')
</t>
<t tx="ekr.20160220050433.123">
def run_all_unit_tests(self):
    '''Run all unit tests in the python-to-coffeescript/test directory.'''
    import unittest
    loader = unittest.TestLoader()
    suite = loader.discover(os.path.abspath('.'),
                            pattern='test*.py',
                            top_level_dir=None)
    unittest.TextTestRunner(verbosity=1).run(suite)
</t>
<t tx="ekr.20160220050433.124">
def scan_command_line(self):
    '''Set ivars from command-line arguments.'''
    # This automatically implements the --help option.
    usage = "usage: python_to_coffeescript.py [options] file1, file2, ..."
    parser = optparse.OptionParser(usage=usage)
    add = parser.add_option
    add('-c', '--config', dest='fn',
        help='full path to configuration file')
    add('-d', '--dir', dest='dir',
        help='full path to the output directory')
    add('-o', '--overwrite', action='store_true', default=False,
        help='overwrite existing .coffee files')
    # add('-t', '--test', action='store_true', default=False,
        # help='run unit tests on startup')
    add('-v', '--verbose', action='store_true', default=False,
        help='verbose output')
    # Parse the options
    options, args = parser.parse_args()
    # Handle the options...
    # self.enable_unit_tests = options.test
    self.overwrite = options.overwrite
    if options.fn:
        self.config_fn = options.fn
    if options.dir:
        dir_ = options.dir
        dir_ = self.finalize(dir_)
        if os.path.exists(dir_):
            self.output_directory = dir_
        else:
            print('--dir: directory does not exist: %s' % dir_)
            print('exiting')
            sys.exit(1)
    # If any files remain, set self.files.
    if args:
        args = [self.finalize(z) for z in args]
        if args:
            self.files = args
</t>
<t tx="ekr.20160220050433.125">
def scan_options(self):
    '''Set all configuration-related ivars.'''
    trace = False
    if not self.config_fn:
        return
    self.parser = parser = self.create_parser()
    s = self.get_config_string()
    self.init_parser(s)
    if self.files:
        files_source = 'command-line'
        files = self.files
    elif parser.has_section('Global'):
        files_source = 'config file'
        files = parser.get('Global', 'files')
        files = [z.strip() for z in files.split('\n') if z.strip()]
    else:
        return
    files2 = []
    for z in files:
        files2.extend(glob.glob(self.finalize(z)))
    self.files = [z for z in files2 if z and os.path.exists(z)]
    if trace:
        print('Files (from %s)...\n' % files_source)
        for z in self.files:
            print(z)
        print('')
    if 'output_directory' in parser.options('Global'):
        s = parser.get('Global', 'output_directory')
        output_dir = self.finalize(s)
        if os.path.exists(output_dir):
            self.output_directory = output_dir
            if self.verbose:
                print('output directory: %s\n' % output_dir)
        else:
            print('output directory not found: %s\n' % output_dir)
            self.output_directory = None # inhibit run().
    if 'prefix_lines' in parser.options('Global'):
        prefix = parser.get('Global', 'prefix_lines')
        self.prefix_lines = prefix.split('\n')
            # The parser does not preserve leading whitespace.
        if trace:
            print('Prefix lines...\n')
            for z in self.prefix_lines:
                print(z)
            print('')
    #
    # self.def_patterns = self.scan_patterns('Def Name Patterns')
    # self.general_patterns = self.scan_patterns('General Patterns')
    # self.make_patterns_dict()
</t>
<t tx="ekr.20160220050433.127">
def create_parser(self):
    '''Create a RawConfigParser and return it.'''
    parser = configparser.RawConfigParser(dict_type=OrderedDict)
        # Requires Python 2.7
    parser.optionxform = str
    return parser
</t>
<t tx="ekr.20160220050433.129">
def get_config_string(self):
    fn = self.finalize(self.config_fn)
    if os.path.exists(fn):
        if self.verbose:
            print('\nconfiguration file: %s\n' % fn)
        f = open(fn, 'r')
        s = f.read()
        f.close()
        return s
    else:
        print('\nconfiguration file not found: %s' % fn)
        return ''
</t>
<t tx="ekr.20160220050433.13">
def truncate(s, n):
    '''Return s truncated to n characters.'''
    return s if len(s) &lt;= n else s[:n-3] + '...'
</t>
<t tx="ekr.20160220050433.130">
def init_parser(self, s):
    '''Add double back-slashes to all patterns starting with '['.'''
    trace = False
    if not s: return
    aList = []
    for s in s.split('\n'):
        if self.is_section_name(s):
            aList.append(s)
        elif s.strip().startswith('['):
            aList.append(r'\\' + s[1:])
            if trace: g.trace('*** escaping:', s)
        else:
            aList.append(s)
    s = '\n'.join(aList) + '\n'
    if trace: g.trace(s)
    file_object = io.StringIO(s)
    self.parser.readfp(file_object)
</t>
<t tx="ekr.20160220050433.131">
def is_section_name(self, s):

    def munge(s):
        return s.strip().lower().replace(' ', '')

    s = s.strip()
    if s.startswith('[') and s.endswith(']'):
        s = munge(s[1: -1])
        for s2 in self.section_names:
            if s == munge(s2):
                return True
    return False
</t>
<t tx="ekr.20160220050433.14">

class CoffeeScriptTraverser(object):
    '''A class to convert python sources to coffeescript sources.'''
    # pylint: disable=consider-using-enumerate
    
    def __init__(self, controller):
        '''Ctor for CoffeeScriptFormatter class.'''
        self.controller = controller
        self.sync_op = None
        self.sync_word = None
    @others
</t>
<t tx="ekr.20160220050433.16">
def format(self, node, tokens):
    '''Format the node (or list of nodes) and its descendants.'''
    self.level = 0
    ts = TokenSync(tokens)
    self.sync_op = ts.sync_op
    self.sync_word = ts.sync_word
    val = self.visit(node)
    return val or ''
</t>
<t tx="ekr.20160220050433.17">
def visit(self, node):
    '''Return the formatted version of an Ast node, or list of Ast nodes.'''
    # g.trace(node.__class__.__name__)
    if isinstance(node, (list, tuple)):
        return ', '.join([self.visit(z) for z in node])
    elif node is None:
        return 'None'
    else:
        assert isinstance(node, ast.AST), node.__class__.__name__
        method_name = 'do_' + node.__class__.__name__
        method = getattr(self, method_name)
        s = method(node)
        # pylint: disable=unidiomatic-typecheck
        assert type(s) == type('abc'), (node, type(s))
        return s
</t>
<t tx="ekr.20160220050433.18">
#
# CoffeeScriptTraverser contexts...
#</t>
<t tx="ekr.20160220050433.188">

class TestClass(object):
    '''A class containing constructs that have caused difficulties.'''
    # pylint: disable=no-member
    # pylint: disable=undefined-variable
    # pylint: disable=no-self-argument
    # pylint: disable=no-method-argument

    @others
</t>
<t tx="ekr.20160220050433.189">
def parse_group(group):
    if len(group) &gt;= 3 and group[-2] == 'as':
        del group[-2:]
    ndots = 0
    i = 0
    while len(group) &gt; i and group[i].startswith('.'):
        ndots += len(group[i])
        i += 1
    assert ''.join(group[: i]) == '.' * ndots, group
    del group[: i]
    assert all(g == '.' for g in group[1:: 2]), group
    return ndots, os.sep.join(group[:: 2])
</t>
<t tx="ekr.20160220050433.19">
# ClassDef(identifier name, expr* bases, stmt* body, expr* decorator_list)

def do_ClassDef(self, node):
    
    self.sync_word('class')
    result = []
    name = node.name # Only a plain string is valid.
    bases = [self.visit(z) for z in node.bases] if node.bases else []
    result.append('\n\n')
    if bases:
        result.append(self.indent('class %s(%s):\n' % (name, ', '.join(bases))))
    else:
        result.append(self.indent('class %s:\n' % name))
    for i, z in enumerate(node.body):
        self.level += 1
        # self.first_statement = i == 0
        result.append(self.visit(z))
        self.level -= 1
    s = ''.join(result)
    g.trace(result[2].rstrip())
    return s
</t>
<t tx="ekr.20160220050433.190">
def return_all(self):
    return all([is_known_type(z) for z in s3.split(',')])
    # return all(['abc'])
</t>
<t tx="ekr.20160220050433.191">
def return_array():
    return f(s[1: -1])
</t>
<t tx="ekr.20160220050433.192">
def return_list(self, a):
    return [a]
</t>
<t tx="ekr.20160220050433.193">
def return_two_lists(s):
    if 1:
        return aList
    else:
        return list(self.regex.finditer(s))
</t>
<t tx="ekr.20160220050433.2">from collections import OrderedDict
    # Requires Python 2.7 or above. Without OrderedDict
    # the configparser will give random order for patterns.

import ast
import glob
import optparse
import os
# import re
import sys
import time
import token
import tokenize
import types
isPython3 = sys.version_info &gt;= (3, 0, 0)
# Avoid try/except here during development.
if isPython3:
    import configparser
    import io
else:
    import ConfigParser as configparser
    import StringIO as io
# try:
    # import ConfigParser as configparser # Python 2
# except ImportError:
    # import configparser # Python 3
# try:
    # import StringIO as io # Python 2
# except ImportError:
    # import io # Python 3
</t>
<t tx="ekr.20160220050433.20">
# FunctionDef(identifier name, arguments args, stmt* body, expr* decorator_list)

def do_FunctionDef(self, node):
    '''Format a FunctionDef node.'''
    self.sync_word('def')
    result = []
    if node.decorator_list:
        for z in node.decorator_list:
            result.append(self.indent('@%s\n' % self.visit(z)))
    name = node.name # Only a plain string is valid.
    args = self.visit(node.args) if node.args else ''
    result.append('\n')
    result.append(self.indent('def %s(%s):\n' % (name, args)))
    for i, z in enumerate(node.body):
        self.level += 1
        # self.first_statement = i == 0
        result.append(self.visit(z))
        self.level -= 1
    s = ''.join(result)
    g.trace(result[1].rstrip())
    return s
</t>
<t tx="ekr.20160220050433.21">
def do_Interactive(self, node):
    for z in node.body:
        self.visit(z)
</t>
<t tx="ekr.20160220050433.22">
def do_Module(self, node):

    return ''.join([self.visit(z) for z in node.body])
</t>
<t tx="ekr.20160220050433.23">
def do_Lambda(self, node):
    return self.indent('lambda %s: %s' % (
        self.visit(node.args),
        self.visit(node.body)))
</t>
<t tx="ekr.20160220050433.24">
#
# CoffeeScriptTraverser expressions...
#

</t>
<t tx="ekr.20160220050433.25">
def do_Expr(self, node):
    '''An outer expression: must be indented.'''
    return self.indent('%s\n' % self.visit(node.value))
</t>
<t tx="ekr.20160220050433.26">
def do_Expression(self, node):
    '''An inner expression: do not indent.'''
    return '%s\n' % self.visit(node.body)
</t>
<t tx="ekr.20160220050433.27">
def do_GeneratorExp(self, node):
    elt = self.visit(node.elt) or ''
    gens = [self.visit(z) for z in node.generators]
    gens = [z if z else '&lt;**None**&gt;' for z in gens] # Kludge: probable bug.
    return '&lt;gen %s for %s&gt;' % (elt, ','.join(gens))
</t>
<t tx="ekr.20160220050433.28">
def do_AugLoad(self, node):
    return 'AugLoad'

def do_Del(self, node):
    return 'Del'

def do_Load(self, node):
    return 'Load'

def do_Param(self, node):
    return 'Param'

def do_Store(self, node):
    return 'Store'
</t>
<t tx="ekr.20160220050433.29">
#
# CoffeeScriptTraverser operands...
#</t>
<t tx="ekr.20160220050433.30">
# arguments = (expr* args, identifier? vararg, identifier? kwarg, expr* defaults)

def do_arguments(self, node):
    '''Format the arguments node.'''
    assert isinstance(node, ast.arguments)
    args = [self.visit(z) for z in node.args]
    defaults = [self.visit(z) for z in node.defaults]
    # Assign default values to the last args.
    args2 = []
    n_plain = len(args) - len(defaults)
    for i in range(len(args)):
        if i &lt; n_plain:
            args2.append(args[i])
        else:
            args2.append('%s=%s' % (args[i], defaults[i - n_plain]))
    # Now add the vararg and kwarg args.
    name = getattr(node, 'vararg', None)
    if name:
        # pylint: disable=no-member
        if isPython3 and isinstance(name, ast.arg):
            name = name.arg
        args2.append('*' + name)
    name = getattr(node, 'kwarg', None)
    if name:
        # pylint: disable=no-member
        if isPython3 and isinstance(name, ast.arg):
            name = name.arg
        args2.append('**' + name)
    return ','.join(args2)
</t>
<t tx="ekr.20160220050433.31">
# Python 3:
# arg = (identifier arg, expr? annotation)

def do_arg(self, node):
    return node.arg
</t>
<t tx="ekr.20160220050433.32">
# Attribute(expr value, identifier attr, expr_context ctx)

def do_Attribute(self, node):
    return '%s.%s' % (
        self.visit(node.value),
        node.attr) # Don't visit node.attr: it is always a string.
</t>
<t tx="ekr.20160220050433.33">
def do_Bytes(self, node): # Python 3.x only.
    return str(node.s)
</t>
<t tx="ekr.20160220050433.34">
# Call(expr func, expr* args, keyword* keywords, expr? starargs, expr? kwargs)

def do_Call(self, node):
    func = self.visit(node.func)
    args = [self.visit(z) for z in node.args]
    for z in node.keywords:
        # Calls f.do_keyword.
        args.append(self.visit(z))
    if getattr(node, 'starargs', None):
        args.append('*%s' % (self.visit(node.starargs)))
    if getattr(node, 'kwargs', None):
        args.append('**%s' % (self.visit(node.kwargs)))
    args = [z for z in args if z] # Kludge: Defensive coding.
    s = '%s(%s)' % (func, ','.join(args))
    g.trace(s)
    return s
</t>
<t tx="ekr.20160220050433.35">
# keyword = (identifier arg, expr value)

def do_keyword(self, node):
    # node.arg is a string.
    value = self.visit(node.value)
    # This is a keyword *arg*, not a Python keyword!
    return '%s=%s' % (node.arg, value)
</t>
<t tx="ekr.20160220050433.36">
def do_comprehension(self, node):
    result = []
    name = self.visit(node.target) # A name.
    it = self.visit(node.iter) # An attribute.
    result.append('%s in %s' % (name, it))
    ifs = [self.visit(z) for z in node.ifs]
    if ifs:
        result.append(' if %s' % (''.join(ifs)))
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.37">
def do_Dict(self, node):
    result = []
    keys = [self.visit(z) for z in node.keys]
    values = [self.visit(z) for z in node.values]
    if len(keys) == len(values):
        # result.append('{\n' if keys else '{')
        result.append('{')
        items = []
        for i in range(len(keys)):
            items.append('%s:%s' % (keys[i], values[i]))
        result.append(', '.join(items))
        result.append('}')
        # result.append(',\n'.join(items))
        # result.append('\n}' if keys else '}')
    else:
        print('Error: f.Dict: len(keys) != len(values)\nkeys: %s\nvals: %s' % (
            repr(keys), repr(values)))
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.38">
def do_Ellipsis(self, node):
    return '...'
</t>
<t tx="ekr.20160220050433.39">
def do_ExtSlice(self, node):
    return ':'.join([self.visit(z) for z in node.dims])
</t>
<t tx="ekr.20160220050433.40">
def do_Index(self, node):
    return self.visit(node.value)
</t>
<t tx="ekr.20160220050433.41">
def do_List(self, node):
    # Not used: list context.
    # self.visit(node.ctx)
    elts = [self.visit(z) for z in node.elts]
    elst = [z for z in elts if z] # Defensive.
    return '[%s]' % ','.join(elts)
</t>
<t tx="ekr.20160220050433.42">
def do_ListComp(self, node):
    elt = self.visit(node.elt)
    gens = [self.visit(z) for z in node.generators]
    gens = [z if z else '&lt;**None**&gt;' for z in gens] # Kludge: probable bug.
    return '%s for %s' % (elt, ''.join(gens))
</t>
<t tx="ekr.20160220050433.43">
def do_Name(self, node):
    return node.id

def do_NameConstant(self, node): # Python 3 only.
    s = repr(node.value)
    return 'bool' if s in ('True', 'False') else s
</t>
<t tx="ekr.20160220050433.44">
def do_Num(self, node):
    return repr(node.n)
</t>
<t tx="ekr.20160220050433.45">
# Python 2.x only

def do_Repr(self, node):
    return 'repr(%s)' % self.visit(node.value)
</t>
<t tx="ekr.20160220050433.46">
def do_Slice(self, node):
    lower, upper, step = '', '', ''
    if getattr(node, 'lower', None) is not None:
        lower = self.visit(node.lower)
    if getattr(node, 'upper', None) is not None:
        upper = self.visit(node.upper)
    if getattr(node, 'step', None) is not None:
        step = self.visit(node.step)
    if step:
        return '%s:%s:%s' % (lower, upper, step)
    else:
        return '%s:%s' % (lower, upper)
</t>
<t tx="ekr.20160220050433.47">
def do_Str(self, node):
    '''A string constant, including docstrings.'''
    # A pretty spectacular hack.
    # We assume docstrings are the first expr following a class or def.
    return repr(node.s)
    # TODO: remove all this.
    # docstring = False
    # if self.first_statement:
        # callers = ''.join([z for z in g.callers(2).split(',') if z != 'visit'])
        # docstring = callers.endswith('do_Expr')
    # if docstring:
        # s = repr(node.s).replace('\\n','\n')
        # if s.startswith('"'):
            # return '""%s""' % s
        # else:
            # return "''%s''" % s
    # else:
        # return repr(node.s)
</t>
<t tx="ekr.20160220050433.48">
# Subscript(expr value, slice slice, expr_context ctx)

def do_Subscript(self, node):
    value = self.visit(node.value)
    the_slice = self.visit(node.slice)
    return '%s[%s]' % (value, the_slice)
</t>
<t tx="ekr.20160220050433.49">
def do_Tuple(self, node):
    elts = [self.visit(z) for z in node.elts]
    return '(%s)' % ', '.join(elts)
</t>
<t tx="ekr.20160220050433.50">
#
# CoffeeScriptTraverser operators...
#</t>
<t tx="ekr.20160220050433.51">
def do_BinOp(self, node):
    return '%s%s%s' % (
        self.visit(node.left),
        self.op_name(node.op),
        self.visit(node.right))
</t>
<t tx="ekr.20160220050433.52">
def do_BoolOp(self, node):
    op_name = self.op_name(node.op)
    values = [self.visit(z) for z in node.values]
    return op_name.join(values)
</t>
<t tx="ekr.20160220050433.53">
def do_Compare(self, node):
    result = []
    lt = self.visit(node.left)
    ops = [self.op_name(z) for z in node.ops]
    comps = [self.visit(z) for z in node.comparators]
    result.append(lt)
    if len(ops) == len(comps):
        for i in range(len(ops)):
            result.append('%s%s' % (ops[i], comps[i]))
    else:
        print('can not happen: ops', repr(ops), 'comparators', repr(comps))
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.54">
def do_UnaryOp(self, node):
    return '%s%s' % (
        self.op_name(node.op),
        self.visit(node.operand))
</t>
<t tx="ekr.20160220050433.55">
def do_IfExp(self, node):
    return '%s if %s else %s ' % (
        self.visit(node.body),
        self.visit(node.test),
        self.visit(node.orelse))
</t>
<t tx="ekr.20160220050433.56">
#
# CoffeeScriptTraverser statements...
#</t>
<t tx="ekr.20160220050433.57">
def do_Assert(self, node):
    
    self.sync_word('assert')
    test = self.visit(node.test)
    if getattr(node, 'msg', None) is not None:
        message = self.visit(node.msg)
        s = self.indent('assert %s, %s\n' % (test, message))
    else:
        s = self.indent('assert %s\n' % test)
    g.trace(s)
    return s
</t>
<t tx="ekr.20160220050433.58">
def do_Assign(self, node):
    
    self.sync_op('=')
    s = self.indent('%s=%s\n' % (
                    '='.join([self.visit(z) for z in node.targets]),
                    self.visit(node.value)))
    g.trace(s)
    return s
</t>
<t tx="ekr.20160220050433.59">
def do_AugAssign(self, node):
    
    op = self.op_name(node.op)
    self.sync_op(op)
    s = self.indent('%s%s=%s\n' % (
        self.visit(node.target),
        op,
        self.visit(node.value)))
    g.trace(s)
    return s
</t>
<t tx="ekr.20160220050433.60">
def do_Break(self, node):
    
    self.sync_word('break')
    return self.indent('break\n')
</t>
<t tx="ekr.20160220050433.61">
def do_Continue(self, node):
    
    self.sync_word('continue')
    return self.indent('continue\n')
</t>
<t tx="ekr.20160220050433.62">
def do_Delete(self, node):
    
    self.sync_word('del')
    targets = [self.visit(z) for z in node.targets]
    return self.indent('del %s\n' % ','.join(targets))
</t>
<t tx="ekr.20160220050433.63">
def do_ExceptHandler(self, node):
    
    self.sync_word('except')
    result = []
    result.append(self.indent('except'))
    if getattr(node, 'type', None):
        result.append(' %s' % self.visit(node.type))
    if getattr(node, 'name', None):
        if isinstance(node.name, ast.AST):
            result.append(' as %s' % self.visit(node.name))
        else:
            result.append(' as %s' % node.name) # Python 3.x.
    result.append(':\n')
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.64">
# Python 2.x only

def do_Exec(self, node):
    
    self.sync_word('exec')
    body = self.visit(node.body)
    args = [] # Globals before locals.
    if getattr(node, 'globals', None):
        args.append(self.visit(node.globals))
    if getattr(node, 'locals', None):
        args.append(self.visit(node.locals))
    if args:
        return self.indent('exec %s in %s\n' % (
            body, ','.join(args)))
    else:
        return self.indent('exec %s\n' % (body))
</t>
<t tx="ekr.20160220050433.65">
def do_For(self, node):
    
    self.sync_word('for')
    result = []
    result.append(self.indent('for %s in %s:\n' % (
        self.visit(node.target),
        self.visit(node.iter))))
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    if node.orelse:
        result.append(self.indent('else:\n'))
        for z in node.orelse:
            self.level += 1
            result.append(self.visit(z))
            self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.66">
def do_Global(self, node):
    
    self.sync_word('global')
    return self.indent('global %s\n' % (
        ','.join(node.names)))
</t>
<t tx="ekr.20160220050433.67">
def do_If(self, node):
    
    self.sync_word('if')
    result = []
    result.append(self.indent('if %s:\n' % (
        self.visit(node.test))))
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    if node.orelse:
        self.sync_word('else')
        result.append(self.indent('else:\n'))
        for z in node.orelse:
            self.level += 1
            result.append(self.visit(z))
            self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.68">
def do_Import(self, node):
    
    self.sync_word('import')
    names = []
    for fn, asname in self.get_import_names(node):
        if asname:
            names.append('%s as %s' % (fn, asname))
        else:
            names.append(fn)
    s = self.indent('import %s\n' % (','.join(names)))
    g.trace(s)
    return s
</t>
<t tx="ekr.20160220050433.69">
def get_import_names(self, node):
    '''Return a list of the the full file names in the import statement.'''
    result = []
    for ast2 in node.names:
        assert isinstance(ast2, ast.alias)
        data = ast2.name, ast2.asname
        result.append(data)
    return result
</t>
<t tx="ekr.20160220050433.7">
#
# Utility functions...
#</t>
<t tx="ekr.20160220050433.70">
def do_ImportFrom(self, node):
    
    
    names = []
    for fn, asname in self.get_import_names(node):
        if asname:
            names.append('%s as %s' % (fn, asname))
        else:
            names.append(fn)
    self.sync_word('from')
    self.sync_word('import')
    s = self.indent('from %s import %s\n' % (
                    node.module,
                    ','.join(names)))
    g.trace(s)
    return s
</t>
<t tx="ekr.20160220050433.71">
def do_Pass(self, node):
    
    self.sync_word('pass')
    return self.indent('pass\n')
</t>
<t tx="ekr.20160220050433.72">
# Python 2.x only

def do_Print(self, node):
    
    self.sync_word('print')
    vals = []
    for z in node.values:
        vals.append(self.visit(z))
    if getattr(node, 'dest', None):
        vals.append('dest=%s' % self.visit(node.dest))
    if getattr(node, 'nl', None):
        if node.nl == 'False':
            vals.append('nl=%s' % node.nl)
    return self.indent('print(%s)\n' % (
        ','.join(vals)))
</t>
<t tx="ekr.20160220050433.73">
def do_Raise(self, node):
    
    self.sync_word('raise')
    args = []
    for attr in ('type', 'inst', 'tback'):
        if getattr(node, attr, None) is not None:
            args.append(self.visit(getattr(node, attr)))
    if args:
        return self.indent('raise %s\n' % (
            ','.join(args)))
    else:
        return self.indent('raise\n')
</t>
<t tx="ekr.20160220050433.74">
def do_Return(self, node):
    
    self.sync_word('return')
    if node.value:
        return self.indent('return %s\n' % (
            self.visit(node.value).strip()))
    else:
        return self.indent('return\n')
</t>
<t tx="ekr.20160220050433.75">
def do_TryExcept(self, node):
    
    self.sync_word('try')
    result = []
    result.append(self.indent('try:\n'))
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    if node.handlers:
        for z in node.handlers:
            result.append(self.visit(z))
    if node.orelse:
        result.append('else:\n')
        for z in node.orelse:
            self.level += 1
            result.append(self.visit(z))
            self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.76">
def do_TryFinally(self, node):
    
    self.sync_word('try')
    result = []
    result.append(self.indent('try:\n'))
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    self.sync_word('finally')
    result.append(self.indent('finally:\n'))
    for z in node.finalbody:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.77">
def do_While(self, node):
    
    self.sync_word('while')
    result = []
    result.append(self.indent('while %s:\n' % (
        self.visit(node.test))))
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    if node.orelse:
        result.append('else:\n')
        for z in node.orelse:
            self.level += 1
            result.append(self.visit(z))
            self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.78">
def do_With(self, node):
    
    self.sync_word('with')
    result = []
    result.append(self.indent('with '))
    if hasattr(node, 'context_expression'):
        result.append(self.visit(node.context_expresssion))
    vars_list = []
    if hasattr(node, 'optional_vars'):
        try:
            for z in node.optional_vars:
                vars_list.append(self.visit(z))
        except TypeError: # Not iterable.
            vars_list.append(self.visit(node.optional_vars))
    result.append(','.join(vars_list))
    result.append(':\n')
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    result.append('\n')
    return ''.join(result)
</t>
<t tx="ekr.20160220050433.79">
def do_Yield(self, node):
    
    self.sync_word('yield')
    if getattr(node, 'value', None):
        return self.indent('yield %s\n' % (
            self.visit(node.value)))
    else:
        return self.indent('yield\n')
</t>
<t tx="ekr.20160220050433.8">
def dump(title, s=None):
    if s:
        print('===== %s...\n%s\n' % (title, s.rstrip()))
    else:
        print('===== %s...\n' % title)
</t>
<t tx="ekr.20160220050433.82">
def indent(self, s):
    '''Return s, properly indented.'''
    assert not s.startswith('\n'), g.callers()
    return '%s%s' % (' ' * 4 * self.level, s)
</t>
<t tx="ekr.20160220050433.83">
def op_name (self,node,strict=True):
    '''Return the print name of an operator node.'''
    d = {
        # Binary operators. 
        'Add':       '+',
        'BitAnd':    '&amp;',
        'BitOr':     '|',
        'BitXor':    '^',
        'Div':       '/',
        'FloorDiv':  '//',
        'LShift':    '&lt;&lt;',
        'Mod':       '%',
        'Mult':      '*',
        'Pow':       '**',
        'RShift':    '&gt;&gt;',
        'Sub':       '-',
        # Boolean operators.
        'And':   ' and ',
        'Or':    ' or ',
        # Comparison operators
        'Eq':    '==',
        'Gt':    '&gt;',
        'GtE':   '&gt;=',
        'In':    ' in ',
        'Is':    ' is ',
        'IsNot': ' is not ',
        'Lt':    '&lt;',
        'LtE':   '&lt;=',
        'NotEq': '!=',
        'NotIn': ' not in ',
        # Context operators.
        'AugLoad':  '&lt;AugLoad&gt;',
        'AugStore': '&lt;AugStore&gt;',
        'Del':      '&lt;Del&gt;',
        'Load':     '&lt;Load&gt;',
        'Param':    '&lt;Param&gt;',
        'Store':    '&lt;Store&gt;',
        # Unary operators.
        'Invert':   '~',
        'Not':      ' not ',
        'UAdd':     '+',
        'USub':     '-',
    }
    kind = node.__class__.__name__
    name = d.get(kind,'&lt;%s&gt;' % kind)
    if strict: assert name, kind
    return name
</t>
<t tx="ekr.20160220050433.86">

class LeoGlobals(object):
    '''A class supporting g.pdb and g.trace for compatibility with Leo.'''
    @others
</t>
<t tx="ekr.20160220050433.87">

class NullObject:
    """
    An object that does nothing, and does it very well.
    From the Python cookbook, recipe 5.23
    """
    def __init__(self, *args, **keys): pass
    def __call__(self, *args, **keys): return self
    def __repr__(self): return "NullObject"
    def __str__(self): return "NullObject"
    def __bool__(self): return False
    def __nonzero__(self): return 0
    def __delattr__(self, attr): return self
    def __getattr__(self, attr): return self
    def __setattr__(self, attr, val): return self
</t>
<t tx="ekr.20160220050433.88">
def _callerName(self, n=1, files=False):
    # print('_callerName: %s %s' % (n,files))
    try: # get the function name from the call stack.
        f1 = sys._getframe(n) # The stack frame, n levels up.
        code1 = f1.f_code # The code object
        name = code1.co_name
        if name == '__init__':
            name = '__init__(%s,line %s)' % (
                self.shortFileName(code1.co_filename), code1.co_firstlineno)
        if files:
            return '%s:%s' % (self.shortFileName(code1.co_filename), name)
        else:
            return name # The code name
    except ValueError:
        # print('g._callerName: ValueError',n)
        return '' # The stack is not deep enough.
    except Exception:
        # es_exception()
        return '' # "&lt;no caller name&gt;"
</t>
<t tx="ekr.20160220050433.89">
def callers(self, n=4, count=0, excludeCaller=True, files=False):
    '''Return a list containing the callers of the function that called g.callerList.

    If the excludeCaller keyword is True (the default), g.callers is not on the list.

    If the files keyword argument is True, filenames are included in the list.
    '''
    # sys._getframe throws ValueError in both cpython and jython if there are less than i entries.
    # The jython stack often has less than 8 entries,
    # so we must be careful to call g._callerName with smaller values of i first.
    result = []
    i = 3 if excludeCaller else 2
    while 1:
        s = self._callerName(i, files=files)
        # print(i,s)
        if s:
            result.append(s)
        if not s or len(result) &gt;= n: break
        i += 1
    result.reverse()
    if count &gt; 0: result = result[: count]
    sep = '\n' if files else ','
    return sep.join(result)
</t>
<t tx="ekr.20160220050433.9">
def dump_dict(title, d):
    '''Dump a dictionary with a header.'''
    dump(title)
    for z in sorted(d):
        print('%30s %s' % (z, d.get(z)))
    print('')
</t>
<t tx="ekr.20160220050433.90">
def cls(self):
    '''Clear the screen.'''
    if sys.platform.lower().startswith('win'):
        os.system('cls')
</t>
<t tx="ekr.20160220050433.91">
def pdb(self):
    try:
        import leo.core.leoGlobals as leo_g
        leo_g.pdb()
    except ImportError:
        import pdb
        pdb.set_trace()
</t>
<t tx="ekr.20160220050433.92">
def shortFileName(self, fileName, n=None):
    if n is None or n &lt; 1:
        return os.path.basename(fileName)
    else:
        return '/'.join(fileName.replace('\\', '/').split('/')[-n:])
</t>
<t tx="ekr.20160220050433.93">
def splitLines(self, s):
    '''Split s into lines, preserving trailing newlines.'''
    return s.splitlines(True) if s else []
</t>
<t tx="ekr.20160220050433.94">
def trace(self, *args, **keys):
    try:
        import leo.core.leoGlobals as leo_g
        leo_g.trace(caller_level=2, *args, **keys)
    except ImportError:
        print(args, keys)
</t>
<t tx="ekr.20160220050745.1">@nocolor-node
@
All parts of this script are distributed under the following copyright. This is intended to be the same as the MIT license, namely that this script is absolutely free, even for commercial use, including resale. There is no GNU-like "copyleft" restriction. This license is compatible with the GPL.

**Copyright 2016 by Edward K. Ream. All Rights Reserved.**

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

**THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.**
</t>
<t tx="ekr.20160220051058.1"></t>
<t tx="ekr.20160220051105.1">@nocolor-node

The problem with an ast-based approach is that it is extremely hard to
reconstruct comments and the spelling of strings. ast_utils.py is an
attempt that looks more completx than the entire CoffeeScriptTokenizer
class! For this reason, I am going to put the CoffeeScriptFormatter class
in the attic.</t>
<t tx="ekr.20160220051359.1"></t>
<t tx="ekr.20160220051359.2">pylint
beautify-tree
sort-lines
# clone-to-at-spot
</t>
<t tx="ekr.20160220051359.4"></t>
<t tx="ekr.20160220051417.1"></t>
<t tx="ekr.20160220051417.10">
def run(self,fn=None):
    
    n, p = 0, c.rootPosition()
    while p:
        if p.h.startswith('@ignore '):
            p.moveToNodeAfterTree()
        elif p.h.startswith('@test '):
            self.nodes.append(p.copy())
            if not fn:
                fn2 = self.clean(p.h)+'.py'
                self.write_file(fn2)
                self.test(fn2)
                self.nodes=[]
            n += 1
            p.moveToThreadNext()
        else:
            p.moveToThreadNext()
    if n == 0:
        print('no @file or @suite nodes found.')
    else:
        if fn:
            self.write_file(fn)
            self.test(fn)
        dest = g.os_path_join(self.path, fn) if fn else self.path
        print('wrote %s test%s to %s' % (n,self.plural(n), dest))
</t>
<t tx="ekr.20160220051417.11">
def plural(self, n):
    return 's' if n &gt; 1 else ''
</t>
<t tx="ekr.20160220051417.12">
def test(self,fn):
    '''Test the newly created file.'''
    import imp
    import sys

    if self.path not in sys.path:
        sys.path.append(self.path)
    assert fn.endswith('.py')
    name = fn[:-3]
    try:
        f,path,desc = imp.find_module(name,[self.path])
        imp.load_module(name,f,path,desc)
        # print('imported %s' % (name))
    except Exception:
        print('can not import: %s' % (name))
        g.es_print_exception()
</t>
<t tx="ekr.20160220051417.13">
def write_file(self,fn):

    assert g.os_path_exists(self.path),self.path
    fn = g.os_path_finalize_join(self.path,fn)
    f = open(fn,'w')
    f.write(self.file_template)
    # g.trace(''.join([z.h for z in self.nodes]))
    for p in self.nodes:
        f.write(self.test_template % (self.clean(p.h),self.get_body(p)))
    f.close()
    g.trace('wrote', fn)
</t>
<t tx="ekr.20160220051417.14">for p1 in c.all_unique_positions():
    if p1.h.startswith('@clean'):
        for p in p1.subtree():
            if (not p.h.strip().startswith('&lt;&lt;') and
                p.b.strip() and not p.b.startswith('\n')
            ):
                g.es_print(repr(p.b[:3]), p.h)
g.es_print('done')
</t>
<t tx="ekr.20160220051417.2">&lt;&lt; docstring &gt;&gt;
# **Note**: this is a Leo script.
# It **does** have access to c, g and p.
import os
import re
@others
test_dir = os.path.dirname(c.fileName())+os.sep+'test'
assert os.path.exists(test_dir), test_dir
assert os.path.isdir(test_dir), test_dir

if 1:
    # Writes each test to a separate file in the test directory.
    TestWriter(c,path=test_dir).run(fn=None)    
if 0:
    # Writes all tests to a single file: test/unit_tests.py
    TestWriter(c,path=test_dir).run(fn='unit_tests.py')
</t>
<t tx="ekr.20160220051417.3">@
@language rest
'''
This script transliterates @test nodes into .py file. The two main ways of
using this script are as follows::

    TestWriter(c,path='test').run(fn='unit_tests.py') # writes one file
    TestWriter(c,path='test').run(fn=None)            # writes separate files.
     
The first writes all tests to test/unit_tests.py; the second writes each
unit test to a separate .py file in the test directory.

The script imports each written file and reports any syntax errors.

This is a straightforward script; it should be easy to modify it to suit
individual needs.

The &lt;\&lt; file_template &gt;&gt; and &lt;\&lt; test_template &gt;&gt; sections in the TestWriter
class determines exactly what this script writes.
'''
</t>
<t tx="ekr.20160220051417.4">

class TestWriter:
    
    &lt;&lt; define file_template &gt;&gt;
    &lt;&lt; define test_template &gt;&gt;

    @others
</t>
<t tx="ekr.20160220051417.5"># Add any other common imports here.

file_template = '''\
import unittest
from make_stub_files import *
'''

file_template = g.adjustTripleString(file_template,c.tab_width)
</t>
<t tx="ekr.20160220051417.6">test_template = '''
class %s (unittest.TestCase):
    def runTest(self):
%s
'''

test_template = g.adjustTripleString(test_template,c.tab_width)
</t>
<t tx="ekr.20160220051417.7">
def __init__(self,c,path=''):
    '''TestWriter ctor.'''
    self.c = c
    load_dir = g.os_path_dirname(c.fileName())
    self.path = g.os_path_finalize_join(load_dir,path)
    self.nodes = []
    assert g.os_path_exists(self.path),self.path
</t>
<t tx="ekr.20160220051417.8">
def clean(self,s):
    '''Munge s so that it can be used as a file name.'''
    result,tag = [],'@test'
    if s.startswith(tag):
        s = s[len(tag):]
    for ch in s.strip():
        if ch.isalnum():
            result.append(ch)
        else:
            result.append('_')
        # elif ch.isspace():
            # result.append('_')
    s = ''.join(result)
    if s.endswith('.py'):
        s = s[:-3]
    if not s.startswith('test'):
        s = 'test_' + s
    return s.replace('__','_').strip()
</t>
<t tx="ekr.20160220051417.9">
def get_body(self, p):
    '''Convert p.b to a valid script.'''
    s_old = p.b
    # Suppress @others but not section references.
    p.b = p.b.replace('@others', '')
    assert p.b.find('@others') == -1
    s = g.getScript(c, p,
                    useSelectedText=False,
                    forcePythonSentinels=True,
                    useSentinels=False)
    p.b = s_old
    s = ''.join([' '*8+z for z in g.splitLines(s) if z.strip()])
            # Add leading indentation needed by test_template.
    return s.rstrip()+'\n'
</t>
<t tx="ekr.20160220055816.3">
def output_time_stamp(self, f):
    '''Put a time-stamp in the output file f.'''
    f.write('# python_to_coffeescript: %s\n' %
        time.strftime("%a %d %b %Y at %H:%M:%S"))
</t>
<t tx="ekr.20160220081725.1"># -*- coding: utf-8 -*-
# https://bitbucket.org/plas/thonny/src/3b71fda7ac0b66d5c475f7a668ffbdc7ae48c2b5/thonny/ast_utils.py?at=master
&lt;&lt; imports &gt;&gt;
# pylint: disable=deprecated-lambda
isPython3 = sys.version_info &gt;= (3, 0, 0)
@others
</t>
<t tx="ekr.20160220081959.1">import ast
import _ast
import io
import sys
import token
import tokenize
# from thonny.common import TextRange
import traceback
</t>
<t tx="ekr.20160220081959.11">def fix_ast_problems(tree, source_lines, tokens):

    # Problem 1:
    # Python parser gives col_offset as offset to its internal UTF-8 byte array
    # I need offsets to chars
    utf8_byte_lines = list(map(lambda line: line.encode("UTF-8"), source_lines))

    # Problem 2:
    # triple-quoted strings have just plain wrong positions: http://bugs.python.org/issue18370
    # Fortunately lexer gives them correct positions
    string_tokens = list(filter(lambda tok: tok.type == token.STRING, tokens))

    # Problem 3:
    # Binary operations have wrong positions: http://bugs.python.org/issue18374
    # Problem 4:
    # Function calls have wrong positions in Python 3.4: http://bugs.python.org/issue21295
    # similar problem is with Attributes and Subscripts

    def fix_node(node):
        for child in _get_ordered_child_nodes(node):
        #for child in ast.iter_child_nodes(node):
            fix_node(child)
        if isinstance(node, ast.Str):
            # fix triple-quote problem
            # get position from tokens
            token = string_tokens.pop(0)
            node.lineno, node.col_offset = token.start
        elif((isinstance(node, ast.Expr) or isinstance(node, ast.Attribute))
            and isinstance(node.value, ast.Str)):
            # they share the wrong offset of their triple-quoted child
            # get position from already fixed child
            # TODO: try whether this works when child is in parentheses
            node.lineno = node.value.lineno
            node.col_offset = node.value.col_offset
        elif(isinstance(node, ast.BinOp)
            and compare_node_positions(node, node.left) &gt; 0):
            # fix binop problem
            # get position from an already fixed child
            node.lineno = node.left.lineno
            node.col_offset = node.left.col_offset
        elif(isinstance(node, ast.Call)
            and compare_node_positions(node, node.func) &gt; 0):
            # Python 3.4 call problem
            # get position from an already fixed child
            node.lineno = node.func.lineno
            node.col_offset = node.func.col_offset
        elif(isinstance(node, ast.Attribute)
            and compare_node_positions(node, node.value) &gt; 0):
            # Python 3.4 attribute problem ...
            node.lineno = node.value.lineno
            node.col_offset = node.value.col_offset
        elif(isinstance(node, ast.Subscript)
            and compare_node_positions(node, node.value) &gt; 0):
            # Python 3.4 Subscript problem ...
            node.lineno = node.value.lineno
            node.col_offset = node.value.col_offset
        else:
            # Let's hope this node has correct lineno, and byte-based col_offset
            # Now compute char-based col_offset
            if hasattr(node, "lineno"):
                byte_line = utf8_byte_lines[node.lineno - 1]
                char_col_offset = len(byte_line[: node.col_offset].decode("UTF-8"))
                node.col_offset = char_col_offset

    fix_node(tree)
</t>
<t tx="ekr.20160220081959.12">def compare_node_positions(n1, n2):
    if n1.lineno &gt; n2.lineno:
        return 1
    elif n1.lineno &lt; n2.lineno:
        return -1
    elif n1.col_offset &gt; n2.col_offset:
        return 1
    elif n2.col_offset &lt; n2.col_offset:
        return -1
    else:
        return 0
</t>
<t tx="ekr.20160220081959.14">def _get_ordered_child_nodes(node):
    if isinstance(node, ast.Dict):
        children = []
        for i in range(len(node.keys)):
            children.append(node.keys[i])
            children.append(node.values[i])
        return children
    elif isinstance(node, ast.Call):
        children = [node.func] + node.args
        for kw in node.keywords:
            children.append(kw.value)
        # TODO: take care of Python 3.5 updates (eg. args=[Starred] and keywords)
        if hasattr(node, "starargs") and node.starargs is not None:
            children.append(node.starargs)
        if hasattr(node, "kwargs") and node.kwargs is not None:
            children.append(node.kwargs)
        children.sort(key=lambda x: (x.lineno, x.col_offset))
        return children
    else:
        return ast.iter_child_nodes(node)
</t>
<t tx="ekr.20160220081959.15">def _tokens_text(tokens):
    return "".join([t.string for t in tokens])
</t>
<t tx="ekr.20160220081959.9">def mark_text_ranges(node, source):
    """
    Node is an AST, source is corresponding source as string.
    Function adds recursively attributes end_lineno and end_col_offset to each node
    which has attributes lineno and col_offset.
    """
    &lt;&lt; mark_text_ranges helpers &gt;&gt;
    ### all_tokens = list(tokenize.tokenize(io.BytesIO(source.encode('utf-8')).readline))
    readline = io.BytesIO(source.encode('utf-8')).readline
    if isPython3:
        all_tokens = list(tokenize.tokenize(readline))
    else:
        all_tokens = []
        def token_eater(t1, t2, t3, t4, t5):
            all_tokens.append(
                Bunch(type=t1, start=(t2, t3), end=(t3, t4), string=t5))
        tokenize.tokenize(readline, token_eater)
    source_lines = source.splitlines(True)
    fix_ast_problems(node, source_lines, all_tokens)
    prelim_end_lineno = len(source_lines)
    prelim_end_col_offset = len(source_lines[len(source_lines) - 1])
    _mark_text_ranges_rec(node, all_tokens, prelim_end_lineno, prelim_end_col_offset)
</t>
<t tx="ekr.20160220084232.1">@others
</t>
<t tx="ekr.20160220084252.1">
def _extract_tokens(tokens, lineno, col_offset, end_lineno, end_col_offset):
    return list(filter((lambda tok: tok.start[0] &gt;= lineno
                               and (tok.start[1] &gt;= col_offset or tok.start[0] &gt; lineno)
                               and tok.end[0] &lt;= end_lineno
                               and (tok.end[1] &lt;= end_col_offset or tok.end[0] &lt; end_lineno)
                               and tok.string != ''),
                       tokens))

</t>
<t tx="ekr.20160220084252.2">
def _mark_text_ranges_rec(node, tokens, prelim_end_lineno, prelim_end_col_offset):
    """
    Returns the earliest starting position found in given tree, 
    this is convenient for internal handling of the siblings
    """
    # set end markers to this node
    if "lineno" in node._attributes and "col_offset" in node._attributes:
        tokens = _extract_tokens(tokens, node.lineno, node.col_offset, prelim_end_lineno, prelim_end_col_offset)
        try:
            tokens = _mark_end_and_return_child_tokens(node, tokens, prelim_end_lineno, prelim_end_col_offset)
        except:
            traceback.print_exc() # TODO: log it somewhere
            # fallback to incorrect marking instead of exception
            node.end_lineno = node.lineno
            node.end_col_offset = node.col_offset + 1
    # mark its children, starting from last one
    # NB! need to sort children because eg. in dict literal all keys come first and then all values
    children = list(_get_ordered_child_nodes(node))
    for child in reversed(children):
        (prelim_end_lineno, prelim_end_col_offset) = \
        _mark_text_ranges_rec(child, tokens, prelim_end_lineno, prelim_end_col_offset)
    if "lineno" in node._attributes and "col_offset" in node._attributes:
        # new "front" is beginning of this node
        prelim_end_lineno = node.lineno
        prelim_end_col_offset = node.col_offset
    return (prelim_end_lineno, prelim_end_col_offset)
</t>
<t tx="ekr.20160220084252.3">
def _strip_trailing_junk_from_expressions(tokens):
    # pylint: disable=no-member
    # the code below works if token.ELLIPSIS does not exist.
    while(tokens[-1].type not in (token.RBRACE, token.RPAR, token.RSQB,
                                  token.NAME, token.NUMBER, token.STRING)
                and not (hasattr(token, "ELLIPSIS") and tokens[-1].type == token.ELLIPSIS)
                and tokens[-1].string not in ")}]"
                or tokens[-1].string in ['and', 'as', 'assert', 'class', 'def', 'del',
                                          'elif', 'else', 'except', 'exec', 'finally',
                                          'for', 'from', 'global', 'if', 'import', 'in',
                                          'is', 'lambda', 'not', 'or', 'try',
                                          'while', 'with', 'yield']):
        del tokens[-1]
</t>
<t tx="ekr.20160220084252.4">def _strip_trailing_extra_closers(tokens, remove_naked_comma):
    level = 0
    for i in range(len(tokens)):
        if tokens[i].string in "({[":
            level += 1
        elif tokens[i].string in ")}]":
            level -= 1
        if level == 0 and tokens[i].string == "," and remove_naked_comma:
            tokens[:] = tokens[0: i]
            return
        if level &lt; 0:
            tokens[:] = tokens[0: i]
            return
</t>
<t tx="ekr.20160220084252.5">def _strip_unclosed_brackets(tokens):
    level = 0
    for i in range(len(tokens) - 1, -1, -1):
        if tokens[i].string in "({[":
            level -= 1
        elif tokens[i].string in ")}]":
            level += 1
        if level &lt; 0:
            tokens[:] = tokens[0: i]
            level = 0 # keep going, there may be more unclosed brackets
</t>
<t tx="ekr.20160220084252.6">def _mark_end_and_return_child_tokens(node, tokens, prelim_end_lineno, prelim_end_col_offset):
    """
    # shortcut
    node.end_lineno = prelim_end_lineno
    node.end_col_offset = prelim_end_col_offset
    return tokens
    """
    # prelim_end_lineno and prelim_end_col_offset are the start of
    # next positioned node or end of source, ie. the suffix of given
    # range may contain keywords, commas and other stuff not belonging to current node
    # Function returns the list of tokens which cover all its children
    if isinstance(node, _ast.stmt):
        # remove empty trailing lines
        while(tokens[-1].type in (tokenize.NL, tokenize.COMMENT, token.NEWLINE, token.INDENT)
               or tokens[-1].string in (":", "else", "elif", "finally", "except")):
            del tokens[-1]
    else:
        _strip_trailing_extra_closers(tokens, not isinstance(node, ast.Tuple))
        _strip_trailing_junk_from_expressions(tokens)
        _strip_unclosed_brackets(tokens)
    # set the end markers of this node
    node.end_lineno = tokens[-1].end[0]
    node.end_col_offset = tokens[-1].end[1]
    # Peel off some trailing tokens which can't be part any
    # positioned child node.
    # TODO: maybe cleaning from parent side is better than
    # _strip_trailing_junk_from_expressions
    # Remove trailing empty parens from no-arg call
    if (isinstance(node, ast.Call)
        and _tokens_text(tokens[-2:]) == "()"):
        del tokens[-2:]
    # Remove trailing full slice
    elif isinstance(node, ast.Subscript):
        if _tokens_text(tokens[-3:]) == "[:]":
            del tokens[-3:]
        elif _tokens_text(tokens[-4:]) == "[::]":
            del tokens[-4:]
    # Attribute name would confuse the "value" of Attribute
    elif isinstance(node, ast.Attribute):
        assert tokens[-1].type == token.NAME
        del tokens[-1]
        _strip_trailing_junk_from_expressions(tokens)
    return tokens
</t>
<t tx="ekr.20160220093908.1">@ From The Python Cookbook: Often we want to just collect a bunch of
stuff together, naming each item of the bunch; a dictionary's OK for
that, but a small do-nothing class is even handier, and prettier to
use.

Create a Bunch whenever you want to group a few variables:

    point = Bunch(datum=y, squared=y*y, coord=x)

You can read/write the named attributes you just created, add others,
del some of them, etc:
    if point.squared &gt; threshold:
        point.isok = True
@c

class Bunch(object):
    """A class that represents a colection of things.

    Especially useful for representing a collection of related variables."""

    def __init__(self, **keywords):
        self.__dict__.update(keywords)

    def __repr__(self):
        return self.toString()

    def ivars(self):
        return sorted(self.__dict__)

    def keys(self):
        return sorted(self.__dict__)

    def toString(self):
        tag = self.__dict__.get('tag')
        entries = ["%s: %s" % (key, str(self.__dict__.get(key)))
            for key in self.ivars() if key != 'tag']
        if tag:
            return "Bunch(tag=%s)...\n%s\n" % (tag, '\n'.join(entries))
        else:
            return "Bunch...\n%s\n" % '\n'.join(entries)
    # Used by new undo code.

    def __setitem__(self, key, value):
        '''Support aBunch[key] = val'''
        return operator.setitem(self.__dict__, key, value)

    def __getitem__(self, key):
        '''Support aBunch[key]'''
        return operator.getitem(self.__dict__, key)

    def get(self, key, theDefault=None):
        return self.__dict__.get(key, theDefault)

bunch = Bunch
</t>
<t tx="ekr.20160220094909.1">
# Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)

def do_Try(self, node): # Python 3

    self.sync_word('try')
    result = []
    result.append(self.indent('try:\n'))
    for z in node.body:
        self.level += 1
        result.append(self.visit(z))
        self.level -= 1
    if node.handlers:
        for z in node.handlers:
            result.append(self.visit(z))
    if node.orelse:
        self.sync_word('else')
        result.append(self.indent('else:\n'))
        for z in node.orelse:
            self.level += 1
            result.append(self.visit(z))
            self.level -= 1
    if node.finalbody:
        self.sync_word('finally')
        result.append(self.indent('finally:\n'))
        for z in node.finalbody:
            self.level += 1
            result.append(self.visit(z))
            self.level -= 1
    return ''.join(result)
</t>
<t tx="ekr.20160220103223.1">

class CoffeeScriptTokenizer:
    '''A token-based Python beautifier.'''
    @others
</t>
<t tx="ekr.20160220103223.10">
def do_endmarker(self):
    '''Handle an endmarker token.'''
    pass
</t>
<t tx="ekr.20160220103223.11">
def do_errortoken(self):
    '''Handle an errortoken token.'''
    # This code is executed for versions of Python earlier than 2.4
    if self.val == '@':
        self.gen_op(self.val)
</t>
<t tx="ekr.20160220103223.12">
def do_dedent(self):
    '''Handle dedent token.'''
    self.level -= 1
    self.lws = self.level * self.tab_width * ' '
    self.gen_line_start()
    # End all classes &amp; defs.
    for state in self.stack.stack:
        if state.kind in ('class', 'def'):
            if state.value &gt;= self.level:
                # g.trace(self.level, 'end', state.kind)
                self.stack.remove(state.kind)
            else:
                break

def do_indent(self):
    '''Handle indent token.'''
    self.level += 1
    self.lws = self.val
    self.gen_line_start()
</t>
<t tx="ekr.20160220103223.13">
def do_name(self):
    '''Handle a name token.'''
    name = self.val
    if name in ('class', 'def'):
        self.gen_class_or_def(name)
    elif name in ('from', 'import'):
        self.gen_import(name)
    elif name == 'self':
        self.gen_self()
    elif self.in_def_line and not self.def_name_seen:
        if name == '__init__':
            name = 'constructor'
        self.gen_word(name)
        if self.stack.has('class'):
            self.gen_op_blank(':')
        else:
            self.gen_op('=')
        self.def_name_seen = True
    elif name in ('and', 'in', 'not', 'not in', 'or'):
        self.gen_word_op(name)
    elif name == 'default':
        # Hard to know where to put a warning comment.
        self.gen_word(name+'_')
    else:
        self.gen_word(name)
</t>
<t tx="ekr.20160220103223.14">
def do_newline(self):
    '''Handle a regular newline.'''
    self.gen_line_end()
</t>
<t tx="ekr.20160220103223.15">
def do_nl(self):
    '''Handle a continuation line.'''
    self.gen_line_end()
</t>
<t tx="ekr.20160220103223.16">
def do_number(self):
    '''Handle a number token.'''
    self.add_token('number', self.val)
</t>
<t tx="ekr.20160220103223.17">
def do_op(self):
    '''Handle an op token.'''
    val = self.val
    if val == '.':
        self.gen_period()
    elif val == '@':
        self.gen_at()
    elif val == ':':
        self.gen_colon()
    elif val == '(':
        self.gen_open_paren()
    elif val == ')':
        self.gen_close_paren()
    elif val == ',':
        self.gen_comma()
    elif val == ';':
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.gen_op_blank(val)
    elif val in '[{':
        # Pep 8: Avoid extraneous whitespace immediately inside
        # parentheses, brackets or braces.
        self.gen_lt(val)
    elif val in ']}':
        self.gen_rt(val)
    elif val == '=':
        # Pep 8: Don't use spaces around the = sign when used to indicate
        # a keyword argument or a default parameter value.
        if self.output_paren_level:
            self.gen_op_no_blanks(val)
        else:
            self.gen_op(val)
    elif val in '~+-':
        self.gen_possible_unary_op(val)
    elif val == '*':
        self.gen_star_op()
    elif val == '**':
        self.gen_star_star_op()
    else:
        # Pep 8: always surround binary operators with a single space.
        # '==','+=','-=','*=','**=','/=','//=','%=','!=','&lt;=','&gt;=','&lt;','&gt;',
        # '^','~','*','**','&amp;','|','/','//',
        # Pep 8: If operators with different priorities are used,
        # consider adding whitespace around the operators with the lowest priority(ies).
        self.gen_op(val)
</t>
<t tx="ekr.20160220103223.18">
def do_string(self):
    '''Handle a 'string' token.'''
    self.add_token('string', self.val)
    if self.val.find('\\\n'):
        self.backslash_seen = False
        # This *does* retain the string's spelling.
    self.gen_blank()
</t>
<t tx="ekr.20160220103223.19">
#
# Output token generators and helpers...
#</t>
<t tx="ekr.20160220103223.2">

class OutputToken(object):
    '''A class representing Output Tokens'''

    def __init__(self, kind, value):
        self.kind = kind
        self.value = value

    def __repr__(self):
        if self.kind == 'line-indent':
            assert not self.value.strip(' ')
            return '%15s %s' % (self.kind, len(self.value))
        else:
            return '%15s %r' % (self.kind, self.value)

    __str__ = __repr__

    def to_string(self):
        '''Convert an output token to a string.'''
        return self.value if g.isString(self.value) else ''
</t>
<t tx="ekr.20160220103223.20">
def add_token(self, kind, value=''):
    '''Add a token to the code list.'''
    token = self.OutputToken(kind, value)
    self.code_list.append(token)
    if kind not in (
        'backslash',
        'blank', 'blank-lines',
        'file-start',
        'line-end', 'line-indent'
    ):
        # g.trace(token,g.callers())
        self.prev_sig_token = token
</t>
<t tx="ekr.20160220103223.21">
# def gen_arg_end(self):
    # '''Add a token indicating the end of an argument list.'''
    # self.add_token('arg-end')

# def gent_arg_start(self):
    # '''Add a token indicating the start of an argument list.'''
    # self.add_token('arg-start')
</t>
<t tx="ekr.20160220103223.22">
def gen_backslash(self):
    '''Add a backslash token and clear .backslash_seen'''
    self.add_token('backslash', '\\')
    self.add_token('line-end', '\n')
    self.gen_line_indent()
    self.backslash_seen = False
</t>
<t tx="ekr.20160220103223.23">
def gen_blank(self):
    '''Add a blank request on the code list.'''
    prev = self.code_list[-1]
    if not prev.kind in (
        'blank', 'blank-lines', 'blank-op',
        'file-start',
        'line-end', 'line-indent',
        'lt', 'op-no-blanks', 'unary-op',
    ):
        self.add_token('blank', ' ')
</t>
<t tx="ekr.20160220103223.24">
def gen_blank_lines(self, n):
    '''
    Add a request for n blank lines to the code list.
    Multiple blank-lines request yield at least the maximum of all requests.
    '''
    self.clean_blank_lines()
    kind = self.code_list[-1].kind
    if kind == 'file-start':
        self.add_token('blank-lines', n)
    else:
        for i in range(0, n + 1):
            self.add_token('line-end', '\n')
        # Retain the token (intention) for debugging.
        self.add_token('blank-lines', n)
        self.gen_line_indent()
</t>
<t tx="ekr.20160220103223.25">
def clean(self, kind):
    '''Remove the last item of token list if it has the given kind.'''
    prev = self.code_list[-1]
    if prev.kind == kind:
        self.code_list.pop()
</t>
<t tx="ekr.20160220103223.26">
def clean_blank_lines(self):
    '''Remove all vestiges of previous lines.'''
    table = ('blank-lines', 'line-end', 'line-indent')
    while self.code_list[-1].kind in table:
        self.code_list.pop()
</t>
<t tx="ekr.20160220103223.27">
def gen_file_end(self):
    '''
    Add a file-end token to the code list.
    Retain exactly one line-end token.
    '''
    self.clean_blank_lines()
    self.add_token('line-end', '\n')
    self.add_token('line-end', '\n')
    self.add_token('file-end')

def gen_file_start(self):
    '''Add a file-start token to the code list and the state stack.'''
    self.add_token('file-start')
    self.stack.push('file-start')
</t>
<t tx="ekr.20160220103223.28">
def gen_line_indent(self, ws=None):
    '''Add a line-indent token if indentation is non-empty.'''
    self.clean('line-indent')
    ws = ws or self.lws
    if ws:
        self.add_token('line-indent', ws)
</t>
<t tx="ekr.20160220103223.29">
def gen_line_end(self):
    '''Add a line-end request to the code list.'''
    prev = self.code_list[-1]
    if prev.kind == 'file-start':
        return
    self.clean('blank') # Important!
    if self.delete_blank_lines:
        self.clean_blank_lines()
    self.clean('line-indent')
    if self.backslash_seen:
        self.gen_backslash()
    self.add_token('line-end', '\n')
    self.gen_line_indent()
        # Add the indentation for all lines
        # until the next indent or unindent token.

def gen_line_start(self):
    '''Add a line-start request to the code list.'''
    self.gen_line_indent()
</t>
<t tx="ekr.20160220103223.3">

class ParseState(object):
    '''A class representing items parse state stack.'''

    def __init__(self, kind, value):
        self.kind = kind
        self.value = value

    def __repr__(self):
        return 'State: %10s %s' % (self.kind, repr(self.value))

    __str__ = __repr__
</t>
<t tx="ekr.20160220103223.30">
def gen_lt(self, s):
    '''Add a left paren to the code list.'''
    assert s in '([{', repr(s)
    self.output_paren_level += 1
    self.clean('blank')
    prev = self.code_list[-1]
    if self.in_def_line:
        self.gen_blank()
        self.add_token('lt', s)
    elif prev.kind in ('op', 'word-op'):
        self.gen_blank()
        if s == '(':
            # g.trace(self.prev_sig_token)
            s = '['
            self.stack.push('tuple', self.output_paren_level)
        self.add_token('lt', s)
    elif prev.kind == 'word':
        # Only suppress blanks before '(' or '[' for non-keyworks.
        if s == '{' or prev.value in ('if', 'else', 'return'):
            self.gen_blank()
        self.add_token('lt', s)
    elif prev.kind == 'op':
        self.gen_op(s)
    else:
        self.gen_op_no_blanks(s)

def gen_rt(self, s):
    '''Add a right paren to the code list.'''
    assert s in ')]}', repr(s)
    self.output_paren_level -= 1
    prev = self.code_list[-1]
    if prev.kind == 'arg-end':
        # Remove a blank token preceding the arg-end token.
        prev = self.code_list.pop()
        self.clean('blank')
        self.code_list.append(prev)
    else:
        self.clean('blank')
        prev = self.code_list[-1]
    if self.stack.has('tuple'):
        # g.trace('line', self.last_line_number, self.output_paren_level + 1)
        state = self.stack.get('tuple')
        if state.value == self.output_paren_level + 1:
            self.add_token('rt', ']')
            self.stack.remove('tuple')
        else:
            self.add_token('rt', s)
    elif s == ')' and prev and prev.kind == 'lt' and prev.value == '(':
        # Remove ()
        self.code_list.pop()
    else:
        self.add_token('rt', s)
</t>
<t tx="ekr.20160220103223.31">
def gen_op(self, s):
    '''Add op token to code list.'''
    assert s and g.isString(s), repr(s)
    self.gen_blank()
    self.add_token('op', s)
    self.gen_blank()

def gen_op_blank(self, s):
    '''Remove a preceding blank token, then add op and blank tokens.'''
    assert s and g.isString(s), repr(s)
    self.clean('blank')
    self.add_token('op', s)
    self.gen_blank()

def gen_op_no_blanks(self, s):
    '''Add an operator *not* surrounded by blanks.'''
    self.clean('blank')
    self.add_token('op-no-blanks', s)
    
def gen_blank_op(self, s):
    '''Add an operator possibly with a preceding blank.'''
    self.gen_blank()
    self.add_token('blank-op', s)
</t>
<t tx="ekr.20160220103223.32">
def gen_possible_unary_op(self, s):
    '''Add a unary or binary op to the token list.'''
    self.clean('blank')
    prev = self.code_list[-1]
    if prev.kind in ('lt', 'op', 'op-no-blanks', 'word-op'):
        self.gen_unary_op(s)
    elif prev.kind == 'word' and prev.value in ('elif', 'if', 'return', 'while'):
        self.gen_unary_op(s)
    else:
        self.gen_op(s)

def gen_unary_op(self, s):
    '''Add an operator request to the code list.'''
    assert s and g.isString(s), repr(s)
    self.gen_blank()
    self.add_token('unary-op', s)
</t>
<t tx="ekr.20160220103223.33">
def gen_star_op(self):
    '''Put a '*' op, with special cases for *args.'''
    val = '*'
    if self.output_paren_level:
        i = len(self.code_list) - 1
        if self.code_list[i].kind == 'blank':
            i -= 1
        token = self.code_list[i]
        if token.kind == 'lt':
            self.gen_op_no_blanks(val)
        elif token.value == ',':
            self.gen_blank()
            self.add_token('op-no-blanks', val)
        else:
            self.gen_op(val)
    else:
        self.gen_op(val)
</t>
<t tx="ekr.20160220103223.34">
def gen_star_star_op(self):
    '''Put a ** operator, with a special case for **kwargs.'''
    val = '**'
    if self.output_paren_level:
        i = len(self.code_list) - 1
        if self.code_list[i].kind == 'blank':
            i -= 1
        token = self.code_list[i]
        if token.value == ',':
            self.gen_blank()
            self.add_token('op-no-blanks', val)
        else:
            self.gen_op(val)
    else:
        self.gen_op(val)
</t>
<t tx="ekr.20160220103223.35">
def gen_word(self, s):
    '''Add a word request to the code list.'''
    assert s and g.isString(s), repr(s)
    self.gen_blank()
    self.add_token('word', s)
    self.gen_blank()

def gen_word_op(self, s):
    '''Add a word-op request to the code list.'''
    assert s and g.isString(s), repr(s)
    self.gen_blank()
    self.add_token('word-op', s)
    self.gen_blank()
</t>
<t tx="ekr.20160220103223.39">
def print_stats(self):
    print('==================== stats')
    print('changed nodes  %s' % self.n_changed_nodes)
    print('tokens         %s' % self.n_input_tokens)
    print('len(code_list) %s' % self.n_output_tokens)
    print('len(s)         %s' % self.n_strings)
    print('parse          %4.2f sec.' % self.parse_time)
    print('tokenize       %4.2f sec.' % self.tokenize_time)
    print('format         %4.2f sec.' % self.beautify_time)
    print('check          %4.2f sec.' % self.check_time)
    print('total          %4.2f sec.' % self.total_time)
</t>
<t tx="ekr.20160220103223.4">
def __init__(self, controller):
    '''Ctor for CoffeeScriptTokenizer class.'''
    self.controller = controller
    # Globals...
    self.code_list = [] # The list of output tokens.
    # The present line and token...
    self.last_line_number = 0
    self.raw_val = None # Raw value for strings, comments.
    self.s = None # The string containing the line.
    self.val = None
    # State vars...
    self.after_self = False
    self.backslash_seen = False
    self.decorator_seen = False
    self.extends_flag = False
    self.in_class_line = False
    self.in_def_line = False
    self.in_import = False
    self.in_list = False
    self.input_paren_level = 0
    self.def_name_seen = False
    self.level = 0 # indentation level.
    self.lws = '' # Leading whitespace.
        # Typically ' '*self.tab_width*self.level,
        # but may be changed for continued lines.
    self.output_paren_level = 0 # Number of unmatched left parens in output.
    self.prev_sig_token = None # Previous non-whitespace token.
    self.stack = None # Stack of ParseState objects, set in format.
    # Settings...
    self.delete_blank_lines = False
    self.tab_width = 4
</t>
<t tx="ekr.20160220103223.40">
def push(self, kind, value=None):
    '''Append a state to the state stack.'''
    trace = False
    self.stack.append(ParseState(kind, value))
    if trace and kind == 'tuple':
        g.trace(kind, value, g.callers(2))
</t>
<t tx="ekr.20160220103223.7">
def format(self, tokens):
    '''The main line of CoffeeScriptTokenizer class.'''
    trace = False
    self.code_list = []
    self.stack = self.StateStack()
    self.gen_file_start()
    for token5tuple in tokens:
        t1, t2, t3, t4, t5 = token5tuple
        srow, scol = t3
        self.kind = token.tok_name[t1].lower()
        self.val = g.toUnicode(t2)
        self.raw_val = g.toUnicode(t5)
        if srow != self.last_line_number:
            # Handle a previous backslash.
            if self.backslash_seen:
                self.gen_backslash()
            # Start a new row.
            raw_val = self.raw_val.rstrip()
            self.backslash_seen = raw_val.endswith('\\')
            # g.trace('backslash_seen',self.backslash_seen)
            if self.output_paren_level &gt; 0:
                s = self.raw_val.rstrip()
                n = g.computeLeadingWhitespaceWidth(s, self.tab_width)
                # This n will be one-too-many if formatting has
                # changed: foo (
                # to:      foo(
                self.gen_line_indent(ws=' ' * n)
                    # Do not set self.lws here!
            self.last_line_number = srow
        if trace: g.trace('%10s %r'% (self.kind,self.val))
        func = getattr(self, 'do_' + self.kind, None)
        if func: func()
    self.gen_file_end()
    return ''.join([z.to_string() for z in self.code_list])
</t>
<t tx="ekr.20160220103223.8">
#
# Input token handlers...
#</t>
<t tx="ekr.20160220103223.9">
def do_comment(self):
    '''Handle a comment token.'''
    raw_val = self.raw_val.rstrip()
    val = self.val.rstrip()
    entire_line = raw_val.lstrip().startswith('#')
    self.backslash_seen = False
        # Putting the comment will put the backslash.
    if entire_line:
        self.clean('line-indent')
        self.add_token('comment', raw_val)
    else:
        self.gen_blank()
        self.add_token('comment', val)
</t>
<t tx="ekr.20160220104209.1">
def isString(self, s):
    '''Return True if s is any string, but not bytes.'''
    if isPython3:
        return type(s) == type('a')
    else:
        return type(s) in types.StringTypes

def isUnicode(self, s):
    '''Return True if s is a unicode string.'''
    if isPython3:
        return type(s) == type('a')
    else:
        return type(s) == types.UnicodeType
</t>
<t tx="ekr.20160220104316.1">
def computeLeadingWhitespace(self, width, tab_width):
    '''Returns optimized whitespace corresponding to width with the indicated tab_width.'''
    if width &lt;= 0:
        return ""
    elif tab_width &gt; 1:
        tabs = int(width / tab_width)
        blanks = int(width % tab_width)
        return ('\t' * tabs) + (' ' * blanks)
    else: # Negative tab width always gets converted to blanks.
        return (' ' * width)
</t>
<t tx="ekr.20160220104523.1">
def toUnicode(self, s, encoding='utf-8', reportErrors=False):
    '''Connvert a non-unicode string with the given encoding to unicode.'''
    trace = False
    if g.isUnicode(s):
        return s
    if not encoding:
        encoding = 'utf-8'
    # These are the only significant calls to s.decode in Leo.
    # Tracing these calls directly yields thousands of calls.
    # Never call g.trace here!
    try:
        s = s.decode(encoding, 'strict')
    except UnicodeError:
        s = s.decode(encoding, 'replace')
        if trace or reportErrors:
            g.trace(g.callers())
            print("toUnicode: Error converting %s... from %s encoding to unicode" % (
                s[: 200], encoding))
    except AttributeError:
        if trace:
            print('toUnicode: AttributeError!: %s' % s)
        # May be a QString.
        s = g.u(s)
    if trace and encoding == 'cp1252':
        print('toUnicode: returns %s' % s)
    return s
</t>
<t tx="ekr.20160220104732.1">
if isPython3:

    def u(self, s):
        return s

    def ue(self, s, encoding):
        return s if g.isUnicode(s) else str(s, encoding)

else:

    def u(self, s):
        return unicode(s)

    def ue(self, s, encoding):
        return unicode(s, encoding)
</t>
<t tx="ekr.20160220105138.1">
def computeLeadingWhitespaceWidth(self, s, tab_width):
    '''Returns optimized whitespace corresponding to width with the indicated tab_width.'''
    w = 0
    for ch in s:
        if ch == ' ':
            w += 1
        elif ch == '\t':
            w += (abs(tab_width) - (w % abs(tab_width)))
        else:
            break
    return w
</t>
<t tx="ekr.20160220110252.1">

class ReadLinesClass:
    """A class whose next method provides a readline method for Python's tokenize module."""

    def __init__(self, s):
        self.lines = s.splitlines(True) if s else []
            # g.splitLines(s)
        self.i = 0

    def next(self):
        if self.i &lt; len(self.lines):
            line = self.lines[self.i]
            self.i += 1
        else:
            line = ''
        # g.trace(repr(line))
        return line

    __next__ = next
</t>
<t tx="ekr.20160220131008.1">@language rest
@wrap

*** Create simple test file.
- Test TokenSync class on *python* output.
    - cv.indent might work with TokenSync class.

Later:
- Handle `elif`
- `###` means something (invalid) to coffeescript
- Remove type hints from arg lists.
- Change `is not` to `isnt`
- Revise TestClass.
- Peephole: last return of def.
</t>
<t tx="ekr.20160220143251.1">
def gen_class_or_def(self, name):
    
    # g.trace(self.level, name)
    self.decorator_seen = False
    if self.stack.has('decorator'):
        self.stack.remove('decorator')
        self.clean_blank_lines()
        self.gen_line_end()
    else:
        self.gen_blank_lines(1)
    self.stack.push(name, self.level)
        # name is 'class' or 'def'
        # do_dedent pops these entries.
    if name == 'def':
        self.in_def_line = True
        self.in_class_line = False
        self.def_name_seen = False
    else:
        self.extends_flag = False
        self.in_class_line = True
        self.gen_word(name)</t>
<t tx="ekr.20160220143629.1">
def gen_colon(self):
    
    val = self.val
    assert val == ':', val
    if self.in_def_line:
        if self.input_paren_level == 0:
            self.in_def_line = False
            self.gen_op('-&gt;')
    elif self.in_class_line:
        if self.input_paren_level == 0:
            self.in_class_line = False
    else:
        pass
        # TODO
        # Some colons are correct.
        # self.gen_op_blank(val)
</t>
<t tx="ekr.20160220143842.1">
def gen_open_paren(self):
    
    val = self.val
    assert val == '(', val
    self.input_paren_level += 1
    if self.in_class_line:
        if not self.extends_flag:
            self.gen_word('extends')
            self.extends_flag = True
    else:
        # Generate a function call or a list.
        self.gen_lt(val)
    self.after_self = False
</t>
<t tx="ekr.20160220143853.1">
def gen_close_paren(self):
    
    val = self.val
    assert val == ')', val
    self.input_paren_level -= 1
    if self.in_class_line:
        self.in_class_line = False
    else:
        self.gen_rt(val)
    self.after_self = False
</t>
<t tx="ekr.20160220144848.1">
def gen_import(self, name):
    '''Convert an import to something that looks like a call.'''
    self.gen_word('pass')
    self.add_token('comment', '# ' + name)
</t>
<t tx="ekr.20160220151109.1">
def has(self, kind):
    '''Return True if state.kind == kind for some ParseState on the stack.'''
    return any([z.kind == kind for z in self.stack])</t>
<t tx="ekr.20160220151729.1">
def gen_self(self):
    if self.in_def_line:
        self.after_self = True
    else:
        self.gen_blank_op('@')
        self.after_self = True
</t>
<t tx="ekr.20160220151922.1">
def gen_period(self):
    
    val = self.val
    assert val == '.', val
    if self.after_self:
        self.after_self = False
    else:
        self.gen_op_no_blanks(val)
</t>
<t tx="ekr.20160220152030.1">
def gen_at(self):
    
    val = self.val
    assert val == '@', val
    if not self.decorator_seen:
        self.gen_blank_lines(1)
        self.decorator_seen = True
    self.gen_op_no_blanks(val)
    self.stack.push('decorator')</t>
<t tx="ekr.20160220160809.1">
def gen_comma(self):
    
    val = self.val
    assert val == ',', val
    if self.after_self:
        self.after_self = False
    else:
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.gen_op_blank(val)
</t>
<t tx="ekr.20160220174952.1">
def remove(self, kind):
    '''Remove the last state on the stack of the given kind.'''
    trace = False
    n = len(self.stack)
    i = n - 1
    found = None
    while 0 &lt;= i:
        state = self.stack[i]
        if state.kind == kind:
            found = state
            self.stack = self.stack[:i] + self.stack[i+1:]
            assert len(self.stack) == n-1, (len(self.stack), n-1)
            break
        i -= 1
    if trace and kind == 'tuple':
        kind = found and found.kind or 'fail'
        value = found and found.value or 'fail'
        g.trace(kind, value, g.callers(2))
</t>
<t tx="ekr.20160220175541.1">
def get(self, kind):
    '''Return the last state of the given kind, leaving the stack unchanged.'''
    n = len(self.stack)
    i = n - 1
    while 0 &lt;= i:
        state = self.stack[i]
        if state.kind == kind:
            return state
        i -= 1
    return None
</t>
<t tx="ekr.20160221024549.1">

class StateStack(object):
    '''
    A class representing a stack of ParseStates and encapsulating various
    operations on same.
    '''
    
    def __init__(self):
        '''Ctor for ParseStack class.'''
        self.stack = []
    @others</t>
<t tx="ekr.20160221025511.1">
def pop(self):
    '''Pop the state on the stack and return it.'''
    return self.stack.pop()</t>
<t tx="ekr.20160221033247.1">
    # Statistics
    self.n_changed_nodes = 0
    self.n_input_tokens = 0
    self.n_output_tokens = 0
    self.n_strings = 0
    self.parse_time = 0.0
    self.tokenize_time = 0.0
    self.beautify_time = 0.0
    self.check_time = 0.0
    self.total_time = 0.0</t>
<t tx="ekr.20160221042509.1"></t>
<t tx="ekr.20160221070924.1">
### Quick Start

1. Put `python_to_coffeescript.py` on your path.

2. Enter a directory containing .py files:

        cd myDirectory
    
3. Generate foo.coffee from foo.py:

        python_to_coffeescript foo.py

4. Look at foo.coffee to see the generated coffeescript code:

        edit foo.coffee

5. (Optional) Run coffeescript itself on the code:

        coffee foo.coffee

6. Regenerate foo.coffee, overwriting the previous .coffee file:

        python_to_coffeescript.py foo.py -o
   
7. (Optional) Specify a configuration file containing defaults:

        python_to_coffeescript.py -c myConfigFile.cfg -o
</t>
<t tx="ekr.20160221071222.1">
### Command-line arguments

    Usage: python_to_coffeescript.py [options] file1, file2, ...
    
    Options:
      -h, --help          show this help message and exit
      -c FN, --config=FN  full path to configuration file
      -d DIR, --dir=DIR   full path to the output directory
      -o, --overwrite     overwrite existing .coffee files
      -v, --verbose       verbose output

*Note*: glob.glob wildcards can be used in file1, file2, ...
</t>
<t tx="ekr.20160221071334.1">
### Overview

This script makes a [coffeescript](http://coffeescript.org/) (.coffee) file in the output directory for each source file listed on the command line (wildcard file names are supported). This script never creates directories automatically, nor does it overwrite .coffee files unless the --overwrite command-line option is in effect.

This script merely converts python syntax to the roughly equivalent coffeescript syntax. It knows nothing about coffeescript semantics. It is intended *only* to help start creating coffeescript code from an existing python code base.

This script already does much of the grunt work of converting python to coffeescript. The script processes itself without error, but coffeescript itself complains about some results.  This is to be expected at this time.
</t>
<t tx="ekr.20160221071354.1">
### Rationale

This script is a *one day prototype*, intended only as a proof of concept. In that sense, it has already succeeded.

The proximate cause for this script were the notes from a [DropBox sprint](https://blogs.dropbox.com/tech/2012/09/dropbox-dives-into-coffeescript/). Coffeescript is obviously successful. Numerous python-to-javascript systems seem unlikely ever to gain traction.

Googling 'python to javascript' or 'python to coffeescript' yields no similar tools, despite many similar searches. This script will be useful to me.

</t>
<t tx="ekr.20160221071408.1">
### Code notes

The present code is based on Leo's token-based beautifier command, with substantial modifications brought about by having to parse the code. Using tokens is reasonable. This approach preserves line breaks, comments and the spelling of strings. On the minus side, it requires ad-hoc parsing of Python, which becomes increasingly difficult as more complex syntactic transformations are attempted.

The initial version of this script was based on ast trees. It is in the attic (the Unused Code section of the python_to_coffeescript.leo). The great disadvantage of parse trees is that it is *extremely* difficult to associate tokens with parse nodes. See [this discussion](http://stackoverflow.com/questions/16748029/how-to-get-source-corresponding-to-a-python-ast-node) and [this proposed solution](https://bitbucket.org/plas/thonny/src/3b71fda7ac0b66d5c475f7a668ffbdc7ae48c2b5/thonny/common.py?at=master). Imo, the solution is not good enough.

**Important**: Even if there were no holes in the ast api, it would still be tricky to associate tokens with parse trees. My present plan is to experiment with using *both* parse-trees and tokens. The idea will be for the ast visitors to **sync** tokens with keywords and strings. This will allow the accumulation of properly-formatted comments preceding keywords, and will allow the perfect reconstruction of strings, something that appears difficult or impossible with a purely parse-tree-oriented approach.

Finally, note that no matter how the code list is generated, it would be possible to use a real peephole pass on it if required.
</t>
<t tx="ekr.20160221071735.1">
### Summary

This has been a success proof of concept. It is surely useful as is.

I have years of experience working with tokens and parse trees. Nevertheless, it remains unclear whether tokens or parse trees will come to dominate the code. Only further work will reveal the best way.

Edward K. Ream  
February 21, 2016
</t>
<t tx="ekr.20160221124544.1">
def advance(self):
    '''Advance one token. Update ivars.'''
    trace = True ; verbose = False
    if not self.tokens:
        return None, None
    token5tuple = self.tokens.pop(0)
    t1, t2, t3, t4, t5 = token5tuple
    srow, scol = t3
    kind = token.tok_name[t1].lower()
    val = g.toUnicode(t2)
    raw_val = g.toUnicode(t5).rstrip()
    if srow != self.last_line_number:
        if trace:
            caller = g.callers(2).split(',')[0].strip()
            g.trace('%16s ===== %s' % (caller, raw_val.rstrip()))
        # Handle a previous backslash.
        if self.backslash_seen:
            self.do_backslash()
        # Start a new row.
        self.backslash_seen = raw_val.endswith('\\')
        if self.output_paren_level &gt; 0:
            s = raw_val
            n = g.computeLeadingWhitespaceWidth(s, self.tab_width)
            # This n will be one-too-many if formatting has
            # changed: foo (
            # to:      foo(
            self.do_line_indent(ws=' ' * n)
                # Do not set self.lws here!
        self.last_line_number = srow
    if trace and verbose and kind == 'name': g.trace(val)
        # g.trace('%10s %s'% (kind,val))
    return kind, val
</t>
<t tx="ekr.20160221143000.1">

class TokenSync(object):
    '''A class to sync and remember tokens.'''
    @others</t>
<t tx="ekr.20160221143402.1">
def __init__(self, tokens):
    '''Ctor for TokenSync class.'''
    self.tab_width = 4
    self.tokens = tokens
    # Accumulation ivars...
    # self.lws = 0
    self.queue = []
        # self.ops = []
        # self.returns = []
        # self.words = []
        # self.ws = []
    # State ivars...
    self.backslash_seen = False
    self.last_line_number = -1
    self.output_paren_level = 0
    # TODO (maybe)...
    # self.kind = None
    # self.raw_val = None
    # self.val = = None
</t>
<t tx="ekr.20160221143755.1">
def do_line_indent(self, ws=None):
    '''Handle line indentation.'''
    # self.clean('line-indent')
    # ws = ws or self.lws
    # if ws:
        # self.add_token('line-indent', ws)
</t>
<t tx="ekr.20160221144446.1">
def do_backslash(self):
    '''Handle a backslash-newline.'''
</t>
<t tx="ekr.20160221145254.1">
def sync_word(self, name):
    '''Advance tokens until the given keyword is found.'''
    trace = True
    kind = 'start'
    while kind:
        kind, val = self.advance()
        self.enqueue(kind, val)
        if kind == 'name' and val == name:
            s = self.dump_queue()
            if trace: g.trace(g.callers(1), '=== FOUND:', s)
            return
    s = self.dump_queue()
    if trace: g.trace(g.callers(1), '=== FAIL:', name, s)
</t>
<t tx="ekr.20160221151640.1">
def sync_op(self, op):
    '''Advance tokens until the given keyword is found.'''
    trace = True
    kind = 'start'
    while kind:
        kind, val = self.advance()
        self.enqueue(kind, val)
        if kind == 'op' and val == op:
            s = self.dump_queue()
            if trace: g.trace(g.callers(1), '=== FOUND:', s)
            return
    s = self.dump_queue()
    if trace: g.trace(g.callers(1), '=== FAIL:', op, s)
</t>
<t tx="ekr.20160221163955.1">
def dump_queue(self):
    '''Return the queued tokens, and clear them.'''
    s = '[%s]' % ' '.join([z.strip() for z in self.queue])
    self.queue = []
    return s
    
def enqueue(self, kind, val):
    '''Queue the token.'''
    if kind == 'string':
        self.queue.append('&lt;str&gt;')
    elif kind in ('name', 'number', 'op'):
        self.queue.append(val or '&lt;empty! %s&gt;' % kind)
</t>
<t tx="ekr.20160222023117.1">'''Test file illustrating difficulties of tokenizing.'''
# lineno: line number of source text (first line is line 1).
# col_offset: the UTF-8 byte offset of the first token that generated the node.
# http://joao.npimentel.net/2015/07/23/python-2-vs-python-3-ast-differences/

a = 1

def spam():
    b = 2
    
class TestClass:
    
    class InnerClass:
        
        def __init__(self):
            pass
            
    def test1():
        pass
        
def eggs():
    pass</t>
<t tx="ekr.20160222023712.1"></t>
<t tx="ekr.20160222030015.1">Lines can be continued using backslash-newlines. Here are the detailed rules from section `2.1.5. Explicit line joining` of the reference. (Same in Python 2 and 3).

Two or more physical lines may be joined into logical lines using backslash characters (\), as follows: when a physical line ends in a backslash that is not part of a string literal or comment, it is joined with the following forming a single logical line, deleting the backslash and the following end-of-line character...

A line ending in a backslash cannot carry a comment. A backslash does not continue a comment. A backslash does not continue a token except for string literals (i.e., tokens other than string literals cannot be split across physical lines using a backslash). A backslash is illegal elsewhere on a line outside a string literal.
</t>
<t tx="ekr.20160222030107.1">http://joao.npimentel.net/2015/07/23/python-2-vs-python-3-ast-differences/
discusses the differences in ast trees between Python 2 and 3. Helpful, but possibly not complete.
</t>
<t tx="ekr.20160222040422.1">@language rest
@wrap

The initial version of python_to_coffeescript.py (the script) used only tokens. This solves all token-related problems, but makes parsing difficult. Conversely, basing the code on ast trees solves all parsing-related problems, but makes recovering token-related information difficult. Yesterday I started experimenting with using parse trees.

This posting gives a preliminary design for a way of associating important token-related data with parse trees. Doing this *cleanly* and *reliably* is far from a trivial project. Imo, it's fascinating and well worth doing for its own sake. I'm starting to have a good feeling about this...

The script needs the following token-related data:

- The **ignored lines** (comment lines and blank lines) that preceed or follow any given **statement line**.

- The **line breaks** occuring within lines. This is not absolutely essential--the script could break lines automatically, but it would be best if the original line breaks were respected.

- The exact spelling of all strings. Essential in general, though perhaps not for python_to_coffeescript.py.

The present plan is as follows:

1. Use *only* the ast.lineno fields and the tokenizer module to recreate token data. The design requires that both the ast.lineno field and Python's tokenizer module are absolutely solid.

2. Ignore the ast.col_offset field. It's notoriously hard to recreate token-related data using col_offset. col_offset is buggy and differs in Python 2 and 3. 

3. Recreate the spelling of strings by traversing the tree in **string order**. That is, we assume that the Str visitor will be called in the order in which strings appear in the source file. This is an important constraint on the traverser class. I *think* it is possible to satisfy this contraint, but I wouldn't be my life on it. Given the list of tokens, we create another list containng only string tokens:

        def tok_name(the_token):
            return token.tok_name[the_token[0]].lower()
    
        string_toks = [z for z in tokens if tok_name(z) == 'string']
    
   The ast.Str visitor gets the strings spelling by popping the next token from the start of the string_toks list.

4. Associate ignored lines with statements by traversing the tree in **line order**. Again, this is a non-trivial constraint on the traversal. Assuming that this constraint can be met, we can *preprocess* the tokens in various ways. For example, it may be useful to tokenize the input line-by-line:

        line_tokens = [list(tokenize.generate_tokens(z)) for z in s.splitlines(True)]
    
   We shall have to munge this list if the ast.lineno is a logical line number instead of a physical line number.

5. Insert line breaks within statements using the line_tokens array. Details omitted. Invention may be required.

### Summary

The overal plan is to sync tokens with statements in the ast tree by preprocessing tokens. There are a number of complicating factors, including continued lines and lines containing multiple statements. Nevertheless, syncing tokens appears more promising than using the ast.col_offset field.

This design is preliminary and experimental. Gotcha's may lurk. Another one-day prototype should prove revealing. I am cautiously optimistic at 4 a.m  ;-)

Edward</t>
<t tx="ekr.20160222041249.1"></t>
</tnodes>
</leo_file>
